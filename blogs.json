{"status":"ok","feed":{"url":"https://medium.com/feed/@edloginova","title":"Stories by Kate Loginova on Medium","link":"https://medium.com/@edloginova?source=rss-362da63ebee0------2","author":"","description":"Stories by Kate Loginova on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*wjCESqwXSODv21OTsomBqQ.jpeg"},"items":[{"title":"Automated Readability Prediction","pubDate":"2019-09-30 12:29:13","link":"https://medium.com/voice-tech-podcast/automated-readability-prediction-71d40b677563?source=rss-362da63ebee0------2","guid":"https://medium.com/p/71d40b677563","author":"Kate Loginova","thumbnail":"","description":"\n<p><em>Hi! I am Kate, a PhD student at the University of Gent, Belgium. I am working with educational data mining, focusing on language learning. After drowning in preparation for yet another literature review, I decided to share some overviews, because I believe this fascinating area of research deserves a bit more attention\u00a0:) I plan to soon publish more posts on other aspects of computer-assisted language learning, such as second language acquisition modelling and automatic question generation. As a part of my project, I am developing methods to assess how readable texts are. So, this is the first post in the series,\u00a0enjoy!</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OH-bTNeYksto0XVubx7TLg.jpeg\"><figcaption><a href=\"https://www.pexels.com/photo/books-sculpture-write-reading-34627/\">https://www.pexels.com/photo/books-sculpture-write-reading-34627/</a></figcaption></figure><a href=\"https://medium.com/media/d6a9e202d4ee773dbc23199fcf6d186c/href\">https://medium.com/media/d6a9e202d4ee773dbc23199fcf6d186c/href</a><h3>Task</h3>\n<p><strong>What is it?</strong> Readability prediction models score texts based on how easily a reader can extract the information from them [1]. This is a rather subjective definition\u200a\u2014\u200abut so are many other widely used terms, such as sentiment in natural language processing. If we have access to pairwise comparisons of texts\u2019 difficulty, we can define readability in a more elegant and strict way: it is the probability the text would be assessed as easier than any other text, by any assessor [2]. Even more pragmatically, it is the proportion of times it was assessed as the easier text in a\u00a0pair.</p>\n<p><strong>Where is it used?</strong> Assessing readability is an important task for media and educational applications because it allows us to tailor content to the readers. For instance, teachers can find interesting texts at an appropriate reading level for language learning classes. Automating readability assessment steps cuts time and effort spent on finding needed content. It can also be a supportive tool during writing, which highlights unreadable passages and suggests how to reformulate them (like Grammarly).</p>\n<p><strong>How is it connect to other concepts?</strong> Readability is closely related to coherence. In [9], it turned out that human annotators tend to give the same rating to these two dimensions of the text. And if you are wondering about the link with machine comprehension, recent research shows that the readability scores do not correlate with machine completion rates [3]. So, machines and humans still see text difficulty in a different way.</p>\n<p><strong>On which granularity level do we do it?</strong> While most of this post is about text readability on the document level, we can also predict it sentence- and word-wise.</p>\n<p>Sentences are used in Computer-Assisted Language Learning (CALL) to generate exercises and vocabulary examples. Sentence readability assessment uses more local features and is actually more difficult than text level [4]. I will briefly describe the approaches for sentence-level prediction further on, but for a more detailed overview of feature importance ranking and model results allow me to refer you to\u00a0[5].</p>\n<p>On a word-level, there is a separate subtask of Complex Word Identification (CWI). It stems from the text simplification task. Usually, deciding which words should be simplified in a given text is the first step in the pipeline. The first shared CWI task was on <a href=\"http://alt.qcri.org/semeval2016/task11/\">SemEval 2016</a>. Among research questions of the challenge were: \u201cto learn which words challenge non-native English speakers and to understand what their traits are\u201d and \u201cto investigate how well one\u2019s individual vocabulary limitations can be predicted from the overall vocabulary limitations of others in the same category\u201d [6]. As you can see, once again a clear link with educational applications.</p>\n<h3>Scores</h3>\n<p>So how do we actually measure readability? The most popular options\u00a0are:</p>\n<ul>\n<li>Grade of school (usually, 5\u201312 grades of American schools) and similar grade levels for other languages</li>\n<li>\u201cEnglish as a second language\u201d levels\u200a\u2014\u200amost widely used are CEFR levels (A1, A2,\u00a0\u2026 C1, C2). You can find their description in <a href=\"https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages\">Wikipedia</a>. They are also rather vague, but textbooks can provide you with a nice annotated dataset.</li>\n<li>The age group of the intended reader\u200a\u2014\u200ain the simplest binary case, is the text written for an adult or child reader? Can be more detailed age bands, as\u00a0well.</li>\n</ul>\n<p>[7] provides a more in-depth discussion about whether reading difficulty corresponds to interval, ordinal or nominal scale data. Interval scale assumes that data is both ordered and evenly spaced, which allows for fewer parameters, but it might be too strong of an assumption. The model operating on ordinal data gets the best score, so the authors conclude that \u201creading difficulty appears to increase steadily but not linearly with grade\u00a0level\u201d.</p>\n<p>Accordingly, we can three different ways to make a prediction with a model [8]. The first one is regression + rounding: the prediction is rounded to its closest integer and clamped to the appropriate range. We can also learn the cut-off separation boundary which bands the ranking scores to levels. And probably the most straightforward is just multi-class classification on the ranking scores. We can also use pairwise ranking: consider pairs of texts and predict which one is more readable. This, too, can be formulated as a classification problem: given two texts, is text 1 more readable than text 2? We will see later how this approach is used for labelling data.</p>\n<h3>Features</h3>\n<p>There is a wide range of features which we can use for readability prediction. [9] and [10] use similar feature categories, which I also follow here. The features can be lexico-semantic, morphological, cognitive, syntactic, semantic, and discourse.</p>\n<h4>Lexico-semantic</h4>\n<p>(relating to words and statistical language\u00a0models)</p>\n<ul>\n<li>\n<strong>language modelling features\u200a</strong>\u2014\u200asuch as the average log-likelihood ratio (discovers keywords which differentiate between corpora) and language model perplexity (on POS and/or word-token, usually 1\u20135 gram). We can also sort words by information gain using language\u00a0models.</li>\n<li>the mean <strong>TF-IDF</strong> value of all tokens in the\u00a0texts.</li>\n<li>relative <strong>frequency </strong>in a large representative corpus. Usually, we would just use the frequency score, but recent research demonstrates that richer representation leads to better results [11]. Even just adding standard deviation already improves the results, but we can also encode separate means for the words of based on their frequency band or cluster. We can also character bigram and trigram frequencies or calculate the proportion of words occurring in a list of the most frequently used\u00a0words.</li>\n<li>the<strong> average number </strong>of words per sentence, the average number of syllables and characters per word, the proportion of words with three or more syllables.</li>\n<li>\n<strong>out-of-vocabulary</strong> (OOV)\u00a0score.</li>\n<li>\n<strong>type-token ratio </strong>(TTR)\u200a\u2014\u200ameasures lexical richness (+ modifications of the formula, such as Root TTR or Corrected TTR).</li>\n<li>\n<strong>word maturity level</strong>\u200a\u2014\u200ait considers how and when a word\u2019s frequency changes with learning stage. This feature is thus well-suited for personalised text difficulty assessment (remember the part about tailoring the content to users?). This feature also accounts for the word\u2019s usage in context by representing it as a feature vector of LSA topics\u00a0[9].</li>\n</ul>\n<h4>Morphological</h4>\n<p>(dealing with morphemes\u200a\u2014\u200asmallest meaningful units of language, for example, suffixes like \u201c-tion\u201d in \u201cablation\u201d)</p>\n<ul><li>suffixes, prefixes\u200a\u2014\u200aproven to be effective for German and some agglutinative languages.</li></ul>\n<h4>Cognitive</h4>\n<p>(based on psycholinguistic and pedagogical research)</p>\n<ul>\n<li>age of acquisition [12], concreteness, degree of polysemy (a word having several meanings).</li>\n<li>Coh-Metrix is a popular software to produce discourse and lexical indices for the text. It can be used, for example, for calculating the level of concreteness or lexical ambiguity of words\u00a0[13].</li>\n</ul>\n<h4>Syntactic</h4>\n<p>(dealing with syntactic parse trees of the sentences)</p>\n<ul>\n<li>simple average count features: average parse tree height, the average number of noun phrases per sentence, the average number of verb phrases per sentence, and an average number of subordinate clauses per sentence [14]. [15] found that the strongest correlation is that between readability and number of verb\u00a0phrases.</li>\n<li>derivative syntactic indicators like clause centre embeddings depth, or number of words per nominal phrase\u00a0[16].</li>\n<li>automatically extracted frequencies of subtree patterns.</li>\n</ul>\n<h4>Semantic</h4>\n<ul>\n<li>\n<strong>semantic role</strong> features: an average number of argument, modifiers, locatives,\u00a0\u2026</li>\n<li>quality of <strong>semantic networks </strong>(which consist of conceptual nodes linked by semantic relations, MultiNet-like) [16]. We can also use them to estimate how probable the sentence is from a semantic point of view and number of connections in them. This way, we can operationalize concepts of polysemy and abstractness [10].</li>\n<li><strong>word embeddings</strong></li>\n<li>\n<strong>higher-level semantics</strong>: use of unusual senses, idioms, or subtle connotation. Here, domain or world knowledge is required to comprehend a\u00a0text.</li>\n</ul>\n<h4>Discourse</h4>\n<p>(for example, use of arguments and connections in the\u00a0text)</p>\n<ul>\n<li>\n<strong>entity </strong>features: entity density, the proportion of transitions in entity extraction, grammatical function and salience entity grid model\u00a0[17].</li>\n<li>number of <strong>coreference </strong>chains per document; average chain span per document; the number of large chain spans within a document\u00a0[18].</li>\n<li>log-likelihood of the language model over discourse relations: text represented as a bag of relations.</li>\n<li>\n<strong>cohesion &amp; coherence\u200a</strong>\u2014\u200adescribe inner logic and structure of the text [9], such as: topic continuity from sentence to sentence: the actual word overlap, average cosine similarity; count the number of connectives included in a text-based on lists or to calculate the causal cohesion by focussing on connectives and causal verbs; the number of pronouns per sentence and the number of definite articles per sentence. An interesting recent development is lexical coherence graph [19]: coherence as semantic connectedness between words modelled by word embeddings. This method allows extracting large subgraphs capturing coherence patterns, increasing interpretability of\u00a0results.</li>\n</ul>\n<h4>Pragmatic</h4>\n<p>contextual or subjective language influenced by genre, e.g.\u00a0sarcasm.</p>\n<p><strong>So which features are the best?</strong> This discussion remains open. The comparison in [18] lists the following features with significant predictive power: POS features, in particular, nouns; verb phrases; average sentence length; language models trained directly on the corpus. However, the conclusions are still inconsistent, because, in another analysis [7], authors conclude that \u201cgrammatical features alone can be effective predictors of readability\u201d.</p>\n<p>There is a <strong>complex interplay between features</strong>. As an example, [15] states that: \u201cthe entity grid factors which individually have a very weak correlation with readability combine well, while adding the three additional discourse features to the likelihood of discourses relations actually worsens performance slightly.\u201d Overall, traditional shallow features are strong single predictors of readability. In the same time, ablation studies show that more innovative features also have a significant impact on the performance [20]. [15] observe that removing syntactic features causes the largest negative impact on performance while removing the cohesion features actually boosts performance. There is some evidence that grammatical and cognitive features may play a more important role in second-language readability prediction.</p>\n<p>Best results are usually obtained by using a <strong>combination of features</strong>. Filter and wrapper feature selection methods (such as forward selection and backward elimination) can be used to find the optimal feature set. They can be combined with genetic algorithms to perform hyperparameter optimisation simultaneously in an efficient way\u00a0[21].</p>\n<p>Concerning <strong>generalizability</strong>, we have two main sources of variety: genre and language. Readability can be measured by different text characteristics depending on the specific language [10], and finding out which features are the most predictive for each language is an active area of research. For example, Arabic, Dari and Pashto are explored in [22], Italian in [23], and Russian in [24] However, [10] also mention that some major features such as lemma frequency are shared across most languages, and [25] claim that almost all the features generalize well across\u00a0corpora.</p>\n<h3>Methods</h3>\n<h4>Classical readability formulas</h4>\n<p>Traditional readability measures use easy-to-compute proxy variables to estimate lexical and syntactic complexity of the sentence [9]. They are usually represented as a simple regression formula with up to five predictors. The predictors are shallow or surface features, such as a number of words. As an example, the most popular formula is the Flesch-Kincaid score:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/447/1*YuHZz_HoiIg5JdctqY141w.png\"><figcaption>The Flesch-Kincaid score</figcaption></figure><p>This formula returns the score from 0 to 100, which is binned according to school levels. For example, the score of 30 corresponds to college graduate level, and 80 to 6th grade. Other popular formulas are <a href=\"https://en.wikipedia.org/wiki/SMOG\">SMOG</a>, <a href=\"https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\">Coleman-Liau index</a>, <a href=\"https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula\">Dale-Chall</a> and <a href=\"https://en.wikipedia.org/wiki/Gunning_fog_index\">Gunning fog</a>. They are implemented in <a href=\"https://pypi.org/project/textstat/\">many libraries</a> and included in text processors.</p>\n<p>These formulas are motivated by the following observations: longer sentences have proven to be more difficult to process than short ones [13], a sentence is difficult to read if the syntactic structure is complex, vocabulary words are not distributed evenly across grade levels\u00a0[26].</p>\n<blockquote>Build better voice apps. Get more articles &amp; interviews from voice technology experts at <a href=\"https://voicetechpodcast.com/\">voicetechpodcast.com</a>\n</blockquote>\n<p>However, these popular and simple formulas have a number of drawbacks.</p>\n<p>1. Advanced features, capturing semantic and syntactic relations, are not considered. Discourse flow, topical dependencies, even the ordering of words and sentences are typically ignored\u00a0[9].</p>\n<p>2. Some of them lack absolute output\u00a0value.</p>\n<p>3. The underlying assumption of regression between readability and the text characteristics has also been criticised.</p>\n<p>4. They are unreliable for short texts (less than 300 words), and allow no noise, assuming well-formed sentences [9]. As such, there is a body of research investigating measures better suited for Web-texts [25].</p>\n<h4>Statistical approaches</h4>\n<p>We can use more advanced models to exploits patterns of word use in language and incorporate more information about the document content. This way we alleviate limitations of the traditional approach. As a nice bonus, many of these models provide a probability distribution of prediction outcomes across all grade models\u00a0[9].</p>\n<p>One of the earliest statistical approaches is a model combining a <strong>unigram language model </strong>and a sentence length one [27]. Such a model assumes that the word sequence is a 1st order Markov process. It was shown to outperform Flesch-Kincaid readability formula.</p>\n<p>This pioneering work was followed by the application of support vector machines (<strong>SVMs</strong>) [14]. Apart from the new method, they also added syntactic features to language model perplexity scores and shallow features. The authors used a traditional formula as a feature, an approach replicated in some studies after that as well. Expanding the work, [15] added discourse relations to the feature set. They observed that readability predictors behave differently depending on the task: readability ranking of classification, but added discourse relations exhibit robustness across these two\u00a0tasks.</p>\n<p><strong>Sentence-level.</strong> As I mentioned in the beginning, we can also have sentence classification, not only text. Graph methods were applied there [28] based on world-coupled TF-IDF matrix. An interesting and important note is that using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences [4]. Sentence readability model can also be used for evaluating text simplification via pairwise ranking [29],\u00a0[30].</p>\n<p><strong>Word-level</strong>. The best G score (harmonic mean of accuracy and recall) in the first shared task on complex word identification problem was 0.774, best F-score is 0.35 [6]. The organisers conclude that \u201cthe most effective way to determine a word\u2019s complexity is by searching for its frequency in corpora.\u201d Concerning the methods, Decision Trees and minimalistic threshold-based strategies perform the best. The task was extended to the multilingual and multi-genre setting [31] in 2018, with the best scores of approximately 0.5\u20130.07 MAE, depending on the\u00a0genre.</p>\n<p><strong>Classification</strong>:</p>\n<ul>\n<li>The usual: Decision Trees, Support Vector Machines, and Logistic Regression. Some authors also experiment with Multi-Layer Perceptron [32].</li>\n<li>Pair-wise ranking with neural models [33], inspired by advances in coherence prediction. The architecture is as follows: 1. run an LSTM over words in sentences to incorporate context information; 2. select the most similar LSTM states in two adjacent sentences to encode the salient semantic concept that relates the sentences and computer their average; 3. apply a convolutional layer to automatically extract and represent patterns of semantic changes in a\u00a0text.</li>\n</ul>\n<p><strong>Regression</strong>: linear regression / logistic regression, including proportional odds model\u00a0[7].</p>\n<p><strong>Clustering</strong>:</p>\n<ul>\n<li>k-nearest neighbours, using deep syntactic and semantic indicator represented by dependency trees and semantic networks\u00a0[16].</li>\n<li>K-means on word embeddings using Euclidean distance [34], combined with SVM regression. Authors mention that the method \u201ccan correctly identify sentence pairs of similar meaning but written in different vocabulary and grammatical structure\u201d, which is an exciting and useful addition.</li>\n</ul>\n<h3>Evaluation</h3>\n<p>To evaluate the performance of our readability prediction models, we can\u00a0use:</p>\n<ul>\n<li>rank-order correlation (typically <strong>Spearman\u2019s rho</strong> or Pearson) between the predicted difficulty levels and the \u2018gold standard\u2019 difficulty levels on the same reference texts\u00a0[9]</li>\n<li>prediction <strong>accuracy </strong>(rounded to the nearest integer level if needed) or <strong>RMSE </strong>(can be understood as the average difference between the predicted grade level and the expected grade level [32]) or <strong>F-score</strong>\u00a0[9]</li>\n</ul>\n<p>[35] found that it is easier to accurately distinguish content at lower levels, and similar observations were made in [24]. On the other hand, classes in the middle of the scale were harder to distinguish in [4]. As such, sometimes the <strong>adjacency </strong>correction is used: for example, we allow the B1 text to be classified as A2 or B2\u00a0[7].</p>\n<h3>Data</h3>\n<p>There is little data with readability labels for generic texts, as most of the existing datasets are based on educational content. [9] provides an overview of the existing publicly available corpora:</p>\n<ul>\n<li>The WeeBit corpus created by [32] is one of the largest datasets for readability analysis. It is composed of articles targeted at readers of different age groups from the Weekly Reader magazine and the BBC-Bitesize website The Weekly Reader data covers non-fictional content for four grade levels, corresponding to children of ages between 7\u20138, 8\u20139, 9\u201310 and 10\u201312 years old. The BBC-Bitesize data covers two grade levels, ages between 11\u201314 and\u00a014\u201316.</li>\n<li>One public resource recently cited in readability evaluations is the collection of texts known as Common Core, comprising 168 docs that span levels roughly corresponding to U.S. grade levels 2\u201312. The passages are tagged by both level and genre, (speech, literature, informative, etc.).</li>\n<li>Another resource is the 114 articles from Encyclopedia Britannica written in two styles, for adults versus children\u00a0[36].</li>\n<li>The Cambridge English Exams are designed for L2 learners specifically and the A2\u2013C2 levels assigned to each reading paper can be treated as the level of reading difficulty of the documents for the L2 learners. [<a href=\"http://www.%20cl.cam.ac.uk/%CB%9Cmx223/cedata.html\">link</a>]</li>\n<li>OneStopEnglish 189 texts, each in three versions (567 in total), freely available [37].</li>\n<li>[2] collected 105 texts from the British National Corpus and Wikipedia in four different genres: administrative, informative, instructive, and miscellaneous. 10907 pairs of texts are labelled with five fine-grained categories by human annotators.</li>\n</ul>\n<p><strong>Labelling. </strong>Absolute scores by human annotators are unavoidably subjective, but we level out differences in reader\u2019s knowledge and attitude by collecting multiple scores per each text. Good news is that according to [2], crowdsourcing is a viable alternative to expert labelling.</p>\n<p>As manual labelling is still expensive even if it is crowdsourced, <strong>automatic construction</strong> is being researched. [38] proposed a framework for automatic generation of large readability corpora. It incorporates a readability filter in combination with a supervised approach, to collect texts at a specific level. The full pipeline: 1. identification of an appropriate set of seed URLs, 2. post-crawl cleaning, 3. readability assessment, 4. near-duplicate detection and removal, and 5. annotation. The authors observe some useful clear patterns which distinguish different levels, such as \u201cpersonal pronouns are more frequent in the lower levels.\u201d [39] continued this line of work, extending the feature set and concluding that readability models generalize adequately to a new\u00a0corpus.</p>\n<h3>Future research</h3>\n<p><strong>User-centric models</strong>: as an example, there is active research in the field of learner-specific word difficulty [40]. By using user-centric models, we can adapt users to content (by providing personalized training) or content to users. Personalised training can include identifying important terms in the text that the user is not likely to know and either explaining or simplifying them. Such systems could \u201cseek optimal strategies and methods for augmenting content or user knowledge in order to actively reduce the \u2018knowledge gap\u2019 between the author and a reader\u201d\u00a0[9].</p>\n<p><strong>Public datasets</strong>. A prominent issue is a lack of significantly-sized, freely available, high-quality corpora for computational readability evaluation. A part of this challenge is r<strong>eadability in Web</strong>: basic readability properties of the Web texts and the influence of readability on user interactions with content, are under-researched.</p>\n<p><strong>Knowledge-based models</strong>: In general, there are few approaches incorporating higher-level semantic and pragmatic features. World knowledge represented by knowledge bases and graphs like DBPedia is a valuable source of information. The dependencies between concepts are especially important in an educational setting because algorithms should understand what a user needs to know before moving on to the next\u00a0concept.</p>\n<h3>Specifics for\u00a0L2</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/1*6gBlChZgLlLNFBClF9r72A.jpeg\"></figure><p>My own projects are currently concerned with the first proposed avenue of research: intelligent tutoring applications that retrieve content on interesting topics in the zone of proximal development. Hence, I am adding some specifics of readability prediction for second language learners.</p>\n<p>Self-directed language learning is explored in [10], and they make a number of interesting remarks. First of all, there is a mismatch between the levels on native and L2 data: \u201cschool grade levels indicating the readability of L1 texts cannot be directly mapped to foreign language learning, but rather need to be learned individually from L2 data\u201d. [8] notice the same and recommend to adapt the pairwise ranking algorithms to ensure that the preference pairs are only created from the same domain. Apart from that, \u201cthe output of readability measures has to be more fine-grained than standard school grades,\u00a0\u2026 readability measures should account for the native language of the learner and should be adapted to groups of users sharing a common mother\u00a0tongue\u201d.</p>\n<p>If you are interested in collaborating on this topic, drop me a\u00a0message!</p>\n<p>Thank you for reading and please don\u2019t hesitate to point out if I missed something\u00a0:)</p>\n<h3>References</h3>\n<p>[1] R. J. Kate <em>et al.</em>, \u201cLearning to predict readability using diverse linguistic features,\u201d <em>Coling 2010\u201323rd Int. Conf. Comput. Linguist. Proc. Conf.</em>, vol. 2, no. August, pp. 546\u2013554,\u00a02010.</p>\n<p>[2] O. De Clercq, V. Hoste, B. Desmet, P. Van Oosten, M. De Cock, and L. Macken, \u201cUsing the crowd for readability prediction,\u201d <em>Nat. Lang. Eng.</em>, vol. 20, no. 3, pp. 293\u2013325,\u00a02014.</p>\n<p>[3] M. Benzahra and F.- Orsay, \u201cMeasuring text readability with machine comprehension\u00a0: a pilot study,\u201d <em>Proc. ofthe Fourteenth Work. Innov. Use ofNLP Build. Educ. Appl.</em>, pp. 412\u2013422,\u00a02019.</p>\n<p>[4] I. Pil\u00e1n, S. Vajjala, and E. Volodina, \u201cA Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity,\u201d 2016.</p>\n<p>[5] F. Dell\u2019Orletta, M. Wieling, G. Venturi, A. Cimino, and S. Montemagni, \u201cAssessing the Readability of Sentences: Which Corpora and Features?,\u201d pp. 163\u2013173,\u00a02015.</p>\n<p>[6] G. H. Paetzold and L. Specia, \u201cSemEval 2016 task 11: Complex word identification,\u201d <em>SemEval 2016\u201310th Int. Work. Semant. Eval. Proc.</em>, pp. 560\u2013569,\u00a02016.</p>\n<p>[7] M. Heilman, K. Collins-Thompson, and M. Eskenazi, \u201cAn analysis of statistical models and features for reading difficulty prediction,\u201d no. June, pp. 71\u201379,\u00a02010.</p>\n<p>[8] M. Xia, E. Kochmar, and T. Briscoe, \u201cText Readability Assessment for Second Language Learners,\u201d pp. 12\u201322,\u00a02016.</p>\n<p>[9] K. Collins-Thompson, \u201cComputational assessment of text readability: A survey of current and future research,\u201d <em>ITL\u200a\u2014\u200aInt. J. Appl. Linguist.</em>, vol. 165, no. 2, pp. 97\u2013135,\u00a02014.</p>\n<p>[10] L. Beinborn, T. Zesch, and I. Gurevych, \u201cTowards fine-grained readability measures for self-directed language learning,\u201d <em>Proc. 1st Work. NLP Comput. Lang. Learn.</em>, vol. 80, no. October, pp. 11\u201319,\u00a02012.</p>\n<p>[11] X. Chen and D. Meurers, \u201cCharacterizing Text Difficulty with Word Frequencies,\u201d pp. 84\u201394,\u00a02016.</p>\n<p>[12] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert, \u201cAge-of-acquisition ratings for 30,000 English words,\u201d <em>Behav. Res. Methods</em>, vol. 44, no. 4, pp. 978\u2013990,\u00a02012.</p>\n<p>[13] A. C. Graesser, D. S. McNamara, M. M. Louwerse, and Z. Cai, \u201cCoh-Metrix: Analysis of text on cohesion and language,\u201d <em>Behav. Res. Methods, Instruments, Comput.</em>, vol. 36, no. 2, pp. 193\u2013202,\u00a02004.</p>\n<p>[14] S. E. Schwarm and M. Ostendorf, \u201cReading level assessment using support vector machines and statistical language models,\u201d <em>ACL-05\u201343rd Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.</em>, no. June, pp. 523\u2013530,\u00a02005.</p>\n<p>[15] E. Pitler and A. Nenkova, \u201cRevisiting readability: A unified framework for predicting text quality,\u201d <em>EMNLP 2008\u20132008 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf. A Meet. SIGDAT, a Spec. Interes. Gr. ACL</em>, no. October, pp. 186\u2013195,\u00a02008.</p>\n<p>[16] T. V. Der Br\u00fcck, S. Hartrumpf, and H. Helbig, \u201cA readability checker with supervised learning using deep indicators,\u201d <em>Inform.</em>, vol. 32, no. 4, pp. 429\u2013435,\u00a02008.</p>\n<p>[17] R. Barzilay and M. Lapata, \u201cModeling Local Coherence: An Entity-Based Approach,\u201d <em>Math. Comput. Model.</em>,\u00a02008.</p>\n<p>[18] L. Feng, M. Jansche, M. Huenerfauth, and N. N. Elhadad, \u201cA comparison of features for automatic readability assessment,\u201d <em>Proc. 23rd Int. Conf. Comput. Linguist. Posters</em>, vol. 2, no. August, pp. 276\u2013284,\u00a02010.</p>\n<p>[19] M. Mesgar and M. Strube, \u201cLexical coherence graph modeling using word embeddings,\u201d <em>2016 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. NAACL HLT 2016\u200a\u2014\u200aProc. Conf.</em>, pp. 1414\u20131423, 2016.</p>\n<p>[20] T. Fran\u00e7ois and E. Miltsakaki, \u201cDo NLP and Machine Learning Improve Traditional Readability Formulas?,\u201d <em>Proc. First Work. Predict. Improv. Text Readability Target Read. Popul.</em>, no. Pitr, pp. 49\u201357,\u00a02012.</p>\n<p>[21] O. De Clercq and V. Hoste, \u201cAll Mixed Up? Finding the Optimal Feature Set for General Readability Prediction and Its Application to English and Dutch,\u201d <em>Comput. Linguist.</em>, 2016.</p>\n<p>[22] E. Salesky and W. Shen, \u201cExploiting Morphological, Grammatical, and Semantic Correlates for Improved Text Difficulty Assessment,\u201d pp. 155\u2013162,\u00a02015.</p>\n<p>[23] L. Forti, A. Milani, L. Piersanti, F. Santarelli, V. Santucci, and S. Spina, \u201cMeasuring Text Complexity for Italian as a Second Language Learning Purposes,\u201d pp. 360\u2013368,\u00a02019.</p>\n<p>[24] R. Reynolds, \u201cInsights from Russian second language readability classification: complexity-dependent training requirements, and feature evaluation of multiple categories,\u201d pp. 289\u2013300,\u00a02016.</p>\n<p>[25] S. Vajjala and D. Meurers, \u201cOn The Applicability of Readability Models to Web Texts,\u201d <em>Proc. 2nd Work. Predict. Improv. Text Readability Target Read. Popul.</em>, pp. 59\u201368,\u00a02013.</p>\n<p>[26] K. Collins-Thompson and J. P. Callan, \u201cA Language Modeling Approach to Predicting Reading Difficulty.,\u201d <em>Proceddings Annu. Conf. North Am. Chapter Assoc. Comput. Linguist.</em>, pp. 193\u2013200,\u00a02004.</p>\n<p>[27] L. Si and J. Callan, \u201cA statistical model for scientific readability,\u201d <em>Int. Conf. Inf. Knowl. Manag. Proc.</em>, pp. 574\u2013576,\u00a02001.</p>\n<p>[28] Z. Jiang, G. Sun, Q. Gu, T. Bai, and D. Chen, \u201cA Graph-based Readability Assessment Method using Word Coupling,\u201d no. September, pp. 411\u2013420,\u00a02015.</p>\n<p>[29] S. Vajjala and D. Meurers, \u201cReadability-based Sentence Ranking for Evaluating Text Simplification,\u201d 2016.</p>\n<p>[30] S. \u0160tajner, S. P. Ponzetto, and H. Stuckenschmidt, \u201cAutomatic assessment of absolute sentence complexity,\u201d <em>IJCAI Int. Jt. Conf. Artif. Intell.</em>, no. October, pp. 4096\u20134102, 2017.</p>\n<p>[31] S. M. Yimam <em>et al.</em>, \u201cA Report on the Complex Word Identification Shared Task 2018,\u201d pp. 66\u201378,\u00a02018.</p>\n<p>[32] S. Vajjala and D. Meurers, \u201cOn Improving the Accuracy of Readability Classification using Insights from Second Language Acquisition,\u201d in <em>The 7th Workshop on the Innovative Use of NLP for Building Educational Application</em>, 2012, pp.\u00a0163\u2013173.</p>\n<p>[33] M. Mesgar and M. Strube, \u201cA Neural Local Coherence Model for Text Quality Assessment,\u201d pp. 4328\u20134339, 2019.</p>\n<p>[34] M. Cha, Y. Gwon, and H. T. Kung, \u201cLanguage Modeling by Clustering with Word Embeddings for Text Readability Assessment,\u201d <em>Proc. 2017 ACM Conf. Inf. Knowl. Manag.\u200a\u2014\u200aCIKM \u201917</em>, pp. 2003\u20132006, 2017.</p>\n<p>[35] J. Nelson, C. Perfetti, D. Liben, and M. Liben, \u201cMeasures of Text Difficulty: Testing their Predictive Value for Grade Levels and Student Performance,\u201d p. 58,\u00a02012.</p>\n<p>[36] R. Barzilay and N. Elhadad, \u201cSentence alignment for monolingual comparable corpora,\u201d pp. 25\u201332,\u00a02003.</p>\n<p>[37] S. Vajjala and I. Lucic, \u201cOneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification,\u201d pp. 297\u2013304,\u00a02018.</p>\n<p>[38] J. Silva, R. Ribeiro, A. Adami, P. Quaresma, and A. Branco, \u201cCrawling by Readability Level,\u201d <em>Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</em>, vol. 9727, pp. 306\u2013318,\u00a02016.</p>\n<p>[39] J. Alberto, W. Filho, R. Wilkens, and A. Villavicencio, \u201cAutomatic Construction of Large Readability Corpora,\u201d pp. 164\u2013173,\u00a02016.</p>\n<p>[40] Y. Ehara, I. Sato, H. Oiwa, and H. Nakagawa, \u201cMining Words in the Minds of Second Language Learners for Learner-specific Word Difficulty,\u201d <em>J. Inf. Process.</em>, vol. 26, no. 0, pp. 267\u2013275,\u00a02018.</p>\n<h3>Something just for\u00a0you</h3>\n<a href=\"https://medium.com/media/7f5cb45fd785102f7af729c560e2de3f/href\">https://medium.com/media/7f5cb45fd785102f7af729c560e2de3f/href</a><a href=\"https://medium.com/media/8a76a1650cf30355bd307195d382558a/href\">https://medium.com/media/8a76a1650cf30355bd307195d382558a/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=71d40b677563\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/voice-tech-podcast/automated-readability-prediction-71d40b677563\">Automated Readability Prediction</a> was originally published in <a href=\"https://medium.com/voice-tech-podcast\">Voice Tech Podcast</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<p><em>Hi! I am Kate, a PhD student at the University of Gent, Belgium. I am working with educational data mining, focusing on language learning. After drowning in preparation for yet another literature review, I decided to share some overviews, because I believe this fascinating area of research deserves a bit more attention\u00a0:) I plan to soon publish more posts on other aspects of computer-assisted language learning, such as second language acquisition modelling and automatic question generation. As a part of my project, I am developing methods to assess how readable texts are. So, this is the first post in the series,\u00a0enjoy!</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OH-bTNeYksto0XVubx7TLg.jpeg\"><figcaption><a href=\"https://www.pexels.com/photo/books-sculpture-write-reading-34627/\">https://www.pexels.com/photo/books-sculpture-write-reading-34627/</a></figcaption></figure><a href=\"https://medium.com/media/d6a9e202d4ee773dbc23199fcf6d186c/href\">https://medium.com/media/d6a9e202d4ee773dbc23199fcf6d186c/href</a><h3>Task</h3>\n<p><strong>What is it?</strong> Readability prediction models score texts based on how easily a reader can extract the information from them [1]. This is a rather subjective definition\u200a\u2014\u200abut so are many other widely used terms, such as sentiment in natural language processing. If we have access to pairwise comparisons of texts\u2019 difficulty, we can define readability in a more elegant and strict way: it is the probability the text would be assessed as easier than any other text, by any assessor [2]. Even more pragmatically, it is the proportion of times it was assessed as the easier text in a\u00a0pair.</p>\n<p><strong>Where is it used?</strong> Assessing readability is an important task for media and educational applications because it allows us to tailor content to the readers. For instance, teachers can find interesting texts at an appropriate reading level for language learning classes. Automating readability assessment steps cuts time and effort spent on finding needed content. It can also be a supportive tool during writing, which highlights unreadable passages and suggests how to reformulate them (like Grammarly).</p>\n<p><strong>How is it connect to other concepts?</strong> Readability is closely related to coherence. In [9], it turned out that human annotators tend to give the same rating to these two dimensions of the text. And if you are wondering about the link with machine comprehension, recent research shows that the readability scores do not correlate with machine completion rates [3]. So, machines and humans still see text difficulty in a different way.</p>\n<p><strong>On which granularity level do we do it?</strong> While most of this post is about text readability on the document level, we can also predict it sentence- and word-wise.</p>\n<p>Sentences are used in Computer-Assisted Language Learning (CALL) to generate exercises and vocabulary examples. Sentence readability assessment uses more local features and is actually more difficult than text level [4]. I will briefly describe the approaches for sentence-level prediction further on, but for a more detailed overview of feature importance ranking and model results allow me to refer you to\u00a0[5].</p>\n<p>On a word-level, there is a separate subtask of Complex Word Identification (CWI). It stems from the text simplification task. Usually, deciding which words should be simplified in a given text is the first step in the pipeline. The first shared CWI task was on <a href=\"http://alt.qcri.org/semeval2016/task11/\">SemEval 2016</a>. Among research questions of the challenge were: \u201cto learn which words challenge non-native English speakers and to understand what their traits are\u201d and \u201cto investigate how well one\u2019s individual vocabulary limitations can be predicted from the overall vocabulary limitations of others in the same category\u201d [6]. As you can see, once again a clear link with educational applications.</p>\n<h3>Scores</h3>\n<p>So how do we actually measure readability? The most popular options\u00a0are:</p>\n<ul>\n<li>Grade of school (usually, 5\u201312 grades of American schools) and similar grade levels for other languages</li>\n<li>\u201cEnglish as a second language\u201d levels\u200a\u2014\u200amost widely used are CEFR levels (A1, A2,\u00a0\u2026 C1, C2). You can find their description in <a href=\"https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages\">Wikipedia</a>. They are also rather vague, but textbooks can provide you with a nice annotated dataset.</li>\n<li>The age group of the intended reader\u200a\u2014\u200ain the simplest binary case, is the text written for an adult or child reader? Can be more detailed age bands, as\u00a0well.</li>\n</ul>\n<p>[7] provides a more in-depth discussion about whether reading difficulty corresponds to interval, ordinal or nominal scale data. Interval scale assumes that data is both ordered and evenly spaced, which allows for fewer parameters, but it might be too strong of an assumption. The model operating on ordinal data gets the best score, so the authors conclude that \u201creading difficulty appears to increase steadily but not linearly with grade\u00a0level\u201d.</p>\n<p>Accordingly, we can three different ways to make a prediction with a model [8]. The first one is regression + rounding: the prediction is rounded to its closest integer and clamped to the appropriate range. We can also learn the cut-off separation boundary which bands the ranking scores to levels. And probably the most straightforward is just multi-class classification on the ranking scores. We can also use pairwise ranking: consider pairs of texts and predict which one is more readable. This, too, can be formulated as a classification problem: given two texts, is text 1 more readable than text 2? We will see later how this approach is used for labelling data.</p>\n<h3>Features</h3>\n<p>There is a wide range of features which we can use for readability prediction. [9] and [10] use similar feature categories, which I also follow here. The features can be lexico-semantic, morphological, cognitive, syntactic, semantic, and discourse.</p>\n<h4>Lexico-semantic</h4>\n<p>(relating to words and statistical language\u00a0models)</p>\n<ul>\n<li>\n<strong>language modelling features\u200a</strong>\u2014\u200asuch as the average log-likelihood ratio (discovers keywords which differentiate between corpora) and language model perplexity (on POS and/or word-token, usually 1\u20135 gram). We can also sort words by information gain using language\u00a0models.</li>\n<li>the mean <strong>TF-IDF</strong> value of all tokens in the\u00a0texts.</li>\n<li>relative <strong>frequency </strong>in a large representative corpus. Usually, we would just use the frequency score, but recent research demonstrates that richer representation leads to better results [11]. Even just adding standard deviation already improves the results, but we can also encode separate means for the words of based on their frequency band or cluster. We can also character bigram and trigram frequencies or calculate the proportion of words occurring in a list of the most frequently used\u00a0words.</li>\n<li>the<strong> average number </strong>of words per sentence, the average number of syllables and characters per word, the proportion of words with three or more syllables.</li>\n<li>\n<strong>out-of-vocabulary</strong> (OOV)\u00a0score.</li>\n<li>\n<strong>type-token ratio </strong>(TTR)\u200a\u2014\u200ameasures lexical richness (+ modifications of the formula, such as Root TTR or Corrected TTR).</li>\n<li>\n<strong>word maturity level</strong>\u200a\u2014\u200ait considers how and when a word\u2019s frequency changes with learning stage. This feature is thus well-suited for personalised text difficulty assessment (remember the part about tailoring the content to users?). This feature also accounts for the word\u2019s usage in context by representing it as a feature vector of LSA topics\u00a0[9].</li>\n</ul>\n<h4>Morphological</h4>\n<p>(dealing with morphemes\u200a\u2014\u200asmallest meaningful units of language, for example, suffixes like \u201c-tion\u201d in \u201cablation\u201d)</p>\n<ul><li>suffixes, prefixes\u200a\u2014\u200aproven to be effective for German and some agglutinative languages.</li></ul>\n<h4>Cognitive</h4>\n<p>(based on psycholinguistic and pedagogical research)</p>\n<ul>\n<li>age of acquisition [12], concreteness, degree of polysemy (a word having several meanings).</li>\n<li>Coh-Metrix is a popular software to produce discourse and lexical indices for the text. It can be used, for example, for calculating the level of concreteness or lexical ambiguity of words\u00a0[13].</li>\n</ul>\n<h4>Syntactic</h4>\n<p>(dealing with syntactic parse trees of the sentences)</p>\n<ul>\n<li>simple average count features: average parse tree height, the average number of noun phrases per sentence, the average number of verb phrases per sentence, and an average number of subordinate clauses per sentence [14]. [15] found that the strongest correlation is that between readability and number of verb\u00a0phrases.</li>\n<li>derivative syntactic indicators like clause centre embeddings depth, or number of words per nominal phrase\u00a0[16].</li>\n<li>automatically extracted frequencies of subtree patterns.</li>\n</ul>\n<h4>Semantic</h4>\n<ul>\n<li>\n<strong>semantic role</strong> features: an average number of argument, modifiers, locatives,\u00a0\u2026</li>\n<li>quality of <strong>semantic networks </strong>(which consist of conceptual nodes linked by semantic relations, MultiNet-like) [16]. We can also use them to estimate how probable the sentence is from a semantic point of view and number of connections in them. This way, we can operationalize concepts of polysemy and abstractness [10].</li>\n<li><strong>word embeddings</strong></li>\n<li>\n<strong>higher-level semantics</strong>: use of unusual senses, idioms, or subtle connotation. Here, domain or world knowledge is required to comprehend a\u00a0text.</li>\n</ul>\n<h4>Discourse</h4>\n<p>(for example, use of arguments and connections in the\u00a0text)</p>\n<ul>\n<li>\n<strong>entity </strong>features: entity density, the proportion of transitions in entity extraction, grammatical function and salience entity grid model\u00a0[17].</li>\n<li>number of <strong>coreference </strong>chains per document; average chain span per document; the number of large chain spans within a document\u00a0[18].</li>\n<li>log-likelihood of the language model over discourse relations: text represented as a bag of relations.</li>\n<li>\n<strong>cohesion &amp; coherence\u200a</strong>\u2014\u200adescribe inner logic and structure of the text [9], such as: topic continuity from sentence to sentence: the actual word overlap, average cosine similarity; count the number of connectives included in a text-based on lists or to calculate the causal cohesion by focussing on connectives and causal verbs; the number of pronouns per sentence and the number of definite articles per sentence. An interesting recent development is lexical coherence graph [19]: coherence as semantic connectedness between words modelled by word embeddings. This method allows extracting large subgraphs capturing coherence patterns, increasing interpretability of\u00a0results.</li>\n</ul>\n<h4>Pragmatic</h4>\n<p>contextual or subjective language influenced by genre, e.g.\u00a0sarcasm.</p>\n<p><strong>So which features are the best?</strong> This discussion remains open. The comparison in [18] lists the following features with significant predictive power: POS features, in particular, nouns; verb phrases; average sentence length; language models trained directly on the corpus. However, the conclusions are still inconsistent, because, in another analysis [7], authors conclude that \u201cgrammatical features alone can be effective predictors of readability\u201d.</p>\n<p>There is a <strong>complex interplay between features</strong>. As an example, [15] states that: \u201cthe entity grid factors which individually have a very weak correlation with readability combine well, while adding the three additional discourse features to the likelihood of discourses relations actually worsens performance slightly.\u201d Overall, traditional shallow features are strong single predictors of readability. In the same time, ablation studies show that more innovative features also have a significant impact on the performance [20]. [15] observe that removing syntactic features causes the largest negative impact on performance while removing the cohesion features actually boosts performance. There is some evidence that grammatical and cognitive features may play a more important role in second-language readability prediction.</p>\n<p>Best results are usually obtained by using a <strong>combination of features</strong>. Filter and wrapper feature selection methods (such as forward selection and backward elimination) can be used to find the optimal feature set. They can be combined with genetic algorithms to perform hyperparameter optimisation simultaneously in an efficient way\u00a0[21].</p>\n<p>Concerning <strong>generalizability</strong>, we have two main sources of variety: genre and language. Readability can be measured by different text characteristics depending on the specific language [10], and finding out which features are the most predictive for each language is an active area of research. For example, Arabic, Dari and Pashto are explored in [22], Italian in [23], and Russian in [24] However, [10] also mention that some major features such as lemma frequency are shared across most languages, and [25] claim that almost all the features generalize well across\u00a0corpora.</p>\n<h3>Methods</h3>\n<h4>Classical readability formulas</h4>\n<p>Traditional readability measures use easy-to-compute proxy variables to estimate lexical and syntactic complexity of the sentence [9]. They are usually represented as a simple regression formula with up to five predictors. The predictors are shallow or surface features, such as a number of words. As an example, the most popular formula is the Flesch-Kincaid score:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/447/1*YuHZz_HoiIg5JdctqY141w.png\"><figcaption>The Flesch-Kincaid score</figcaption></figure><p>This formula returns the score from 0 to 100, which is binned according to school levels. For example, the score of 30 corresponds to college graduate level, and 80 to 6th grade. Other popular formulas are <a href=\"https://en.wikipedia.org/wiki/SMOG\">SMOG</a>, <a href=\"https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\">Coleman-Liau index</a>, <a href=\"https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula\">Dale-Chall</a> and <a href=\"https://en.wikipedia.org/wiki/Gunning_fog_index\">Gunning fog</a>. They are implemented in <a href=\"https://pypi.org/project/textstat/\">many libraries</a> and included in text processors.</p>\n<p>These formulas are motivated by the following observations: longer sentences have proven to be more difficult to process than short ones [13], a sentence is difficult to read if the syntactic structure is complex, vocabulary words are not distributed evenly across grade levels\u00a0[26].</p>\n<blockquote>Build better voice apps. Get more articles &amp; interviews from voice technology experts at <a href=\"https://voicetechpodcast.com/\">voicetechpodcast.com</a>\n</blockquote>\n<p>However, these popular and simple formulas have a number of drawbacks.</p>\n<p>1. Advanced features, capturing semantic and syntactic relations, are not considered. Discourse flow, topical dependencies, even the ordering of words and sentences are typically ignored\u00a0[9].</p>\n<p>2. Some of them lack absolute output\u00a0value.</p>\n<p>3. The underlying assumption of regression between readability and the text characteristics has also been criticised.</p>\n<p>4. They are unreliable for short texts (less than 300 words), and allow no noise, assuming well-formed sentences [9]. As such, there is a body of research investigating measures better suited for Web-texts [25].</p>\n<h4>Statistical approaches</h4>\n<p>We can use more advanced models to exploits patterns of word use in language and incorporate more information about the document content. This way we alleviate limitations of the traditional approach. As a nice bonus, many of these models provide a probability distribution of prediction outcomes across all grade models\u00a0[9].</p>\n<p>One of the earliest statistical approaches is a model combining a <strong>unigram language model </strong>and a sentence length one [27]. Such a model assumes that the word sequence is a 1st order Markov process. It was shown to outperform Flesch-Kincaid readability formula.</p>\n<p>This pioneering work was followed by the application of support vector machines (<strong>SVMs</strong>) [14]. Apart from the new method, they also added syntactic features to language model perplexity scores and shallow features. The authors used a traditional formula as a feature, an approach replicated in some studies after that as well. Expanding the work, [15] added discourse relations to the feature set. They observed that readability predictors behave differently depending on the task: readability ranking of classification, but added discourse relations exhibit robustness across these two\u00a0tasks.</p>\n<p><strong>Sentence-level.</strong> As I mentioned in the beginning, we can also have sentence classification, not only text. Graph methods were applied there [28] based on world-coupled TF-IDF matrix. An interesting and important note is that using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences [4]. Sentence readability model can also be used for evaluating text simplification via pairwise ranking [29],\u00a0[30].</p>\n<p><strong>Word-level</strong>. The best G score (harmonic mean of accuracy and recall) in the first shared task on complex word identification problem was 0.774, best F-score is 0.35 [6]. The organisers conclude that \u201cthe most effective way to determine a word\u2019s complexity is by searching for its frequency in corpora.\u201d Concerning the methods, Decision Trees and minimalistic threshold-based strategies perform the best. The task was extended to the multilingual and multi-genre setting [31] in 2018, with the best scores of approximately 0.5\u20130.07 MAE, depending on the\u00a0genre.</p>\n<p><strong>Classification</strong>:</p>\n<ul>\n<li>The usual: Decision Trees, Support Vector Machines, and Logistic Regression. Some authors also experiment with Multi-Layer Perceptron [32].</li>\n<li>Pair-wise ranking with neural models [33], inspired by advances in coherence prediction. The architecture is as follows: 1. run an LSTM over words in sentences to incorporate context information; 2. select the most similar LSTM states in two adjacent sentences to encode the salient semantic concept that relates the sentences and computer their average; 3. apply a convolutional layer to automatically extract and represent patterns of semantic changes in a\u00a0text.</li>\n</ul>\n<p><strong>Regression</strong>: linear regression / logistic regression, including proportional odds model\u00a0[7].</p>\n<p><strong>Clustering</strong>:</p>\n<ul>\n<li>k-nearest neighbours, using deep syntactic and semantic indicator represented by dependency trees and semantic networks\u00a0[16].</li>\n<li>K-means on word embeddings using Euclidean distance [34], combined with SVM regression. Authors mention that the method \u201ccan correctly identify sentence pairs of similar meaning but written in different vocabulary and grammatical structure\u201d, which is an exciting and useful addition.</li>\n</ul>\n<h3>Evaluation</h3>\n<p>To evaluate the performance of our readability prediction models, we can\u00a0use:</p>\n<ul>\n<li>rank-order correlation (typically <strong>Spearman\u2019s rho</strong> or Pearson) between the predicted difficulty levels and the \u2018gold standard\u2019 difficulty levels on the same reference texts\u00a0[9]</li>\n<li>prediction <strong>accuracy </strong>(rounded to the nearest integer level if needed) or <strong>RMSE </strong>(can be understood as the average difference between the predicted grade level and the expected grade level [32]) or <strong>F-score</strong>\u00a0[9]</li>\n</ul>\n<p>[35] found that it is easier to accurately distinguish content at lower levels, and similar observations were made in [24]. On the other hand, classes in the middle of the scale were harder to distinguish in [4]. As such, sometimes the <strong>adjacency </strong>correction is used: for example, we allow the B1 text to be classified as A2 or B2\u00a0[7].</p>\n<h3>Data</h3>\n<p>There is little data with readability labels for generic texts, as most of the existing datasets are based on educational content. [9] provides an overview of the existing publicly available corpora:</p>\n<ul>\n<li>The WeeBit corpus created by [32] is one of the largest datasets for readability analysis. It is composed of articles targeted at readers of different age groups from the Weekly Reader magazine and the BBC-Bitesize website The Weekly Reader data covers non-fictional content for four grade levels, corresponding to children of ages between 7\u20138, 8\u20139, 9\u201310 and 10\u201312 years old. The BBC-Bitesize data covers two grade levels, ages between 11\u201314 and\u00a014\u201316.</li>\n<li>One public resource recently cited in readability evaluations is the collection of texts known as Common Core, comprising 168 docs that span levels roughly corresponding to U.S. grade levels 2\u201312. The passages are tagged by both level and genre, (speech, literature, informative, etc.).</li>\n<li>Another resource is the 114 articles from Encyclopedia Britannica written in two styles, for adults versus children\u00a0[36].</li>\n<li>The Cambridge English Exams are designed for L2 learners specifically and the A2\u2013C2 levels assigned to each reading paper can be treated as the level of reading difficulty of the documents for the L2 learners. [<a href=\"http://www.%20cl.cam.ac.uk/%CB%9Cmx223/cedata.html\">link</a>]</li>\n<li>OneStopEnglish 189 texts, each in three versions (567 in total), freely available [37].</li>\n<li>[2] collected 105 texts from the British National Corpus and Wikipedia in four different genres: administrative, informative, instructive, and miscellaneous. 10907 pairs of texts are labelled with five fine-grained categories by human annotators.</li>\n</ul>\n<p><strong>Labelling. </strong>Absolute scores by human annotators are unavoidably subjective, but we level out differences in reader\u2019s knowledge and attitude by collecting multiple scores per each text. Good news is that according to [2], crowdsourcing is a viable alternative to expert labelling.</p>\n<p>As manual labelling is still expensive even if it is crowdsourced, <strong>automatic construction</strong> is being researched. [38] proposed a framework for automatic generation of large readability corpora. It incorporates a readability filter in combination with a supervised approach, to collect texts at a specific level. The full pipeline: 1. identification of an appropriate set of seed URLs, 2. post-crawl cleaning, 3. readability assessment, 4. near-duplicate detection and removal, and 5. annotation. The authors observe some useful clear patterns which distinguish different levels, such as \u201cpersonal pronouns are more frequent in the lower levels.\u201d [39] continued this line of work, extending the feature set and concluding that readability models generalize adequately to a new\u00a0corpus.</p>\n<h3>Future research</h3>\n<p><strong>User-centric models</strong>: as an example, there is active research in the field of learner-specific word difficulty [40]. By using user-centric models, we can adapt users to content (by providing personalized training) or content to users. Personalised training can include identifying important terms in the text that the user is not likely to know and either explaining or simplifying them. Such systems could \u201cseek optimal strategies and methods for augmenting content or user knowledge in order to actively reduce the \u2018knowledge gap\u2019 between the author and a reader\u201d\u00a0[9].</p>\n<p><strong>Public datasets</strong>. A prominent issue is a lack of significantly-sized, freely available, high-quality corpora for computational readability evaluation. A part of this challenge is r<strong>eadability in Web</strong>: basic readability properties of the Web texts and the influence of readability on user interactions with content, are under-researched.</p>\n<p><strong>Knowledge-based models</strong>: In general, there are few approaches incorporating higher-level semantic and pragmatic features. World knowledge represented by knowledge bases and graphs like DBPedia is a valuable source of information. The dependencies between concepts are especially important in an educational setting because algorithms should understand what a user needs to know before moving on to the next\u00a0concept.</p>\n<h3>Specifics for\u00a0L2</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/1*6gBlChZgLlLNFBClF9r72A.jpeg\"></figure><p>My own projects are currently concerned with the first proposed avenue of research: intelligent tutoring applications that retrieve content on interesting topics in the zone of proximal development. Hence, I am adding some specifics of readability prediction for second language learners.</p>\n<p>Self-directed language learning is explored in [10], and they make a number of interesting remarks. First of all, there is a mismatch between the levels on native and L2 data: \u201cschool grade levels indicating the readability of L1 texts cannot be directly mapped to foreign language learning, but rather need to be learned individually from L2 data\u201d. [8] notice the same and recommend to adapt the pairwise ranking algorithms to ensure that the preference pairs are only created from the same domain. Apart from that, \u201cthe output of readability measures has to be more fine-grained than standard school grades,\u00a0\u2026 readability measures should account for the native language of the learner and should be adapted to groups of users sharing a common mother\u00a0tongue\u201d.</p>\n<p>If you are interested in collaborating on this topic, drop me a\u00a0message!</p>\n<p>Thank you for reading and please don\u2019t hesitate to point out if I missed something\u00a0:)</p>\n<h3>References</h3>\n<p>[1] R. J. Kate <em>et al.</em>, \u201cLearning to predict readability using diverse linguistic features,\u201d <em>Coling 2010\u201323rd Int. Conf. Comput. Linguist. Proc. Conf.</em>, vol. 2, no. August, pp. 546\u2013554,\u00a02010.</p>\n<p>[2] O. De Clercq, V. Hoste, B. Desmet, P. Van Oosten, M. De Cock, and L. Macken, \u201cUsing the crowd for readability prediction,\u201d <em>Nat. Lang. Eng.</em>, vol. 20, no. 3, pp. 293\u2013325,\u00a02014.</p>\n<p>[3] M. Benzahra and F.- Orsay, \u201cMeasuring text readability with machine comprehension\u00a0: a pilot study,\u201d <em>Proc. ofthe Fourteenth Work. Innov. Use ofNLP Build. Educ. Appl.</em>, pp. 412\u2013422,\u00a02019.</p>\n<p>[4] I. Pil\u00e1n, S. Vajjala, and E. Volodina, \u201cA Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity,\u201d 2016.</p>\n<p>[5] F. Dell\u2019Orletta, M. Wieling, G. Venturi, A. Cimino, and S. Montemagni, \u201cAssessing the Readability of Sentences: Which Corpora and Features?,\u201d pp. 163\u2013173,\u00a02015.</p>\n<p>[6] G. H. Paetzold and L. Specia, \u201cSemEval 2016 task 11: Complex word identification,\u201d <em>SemEval 2016\u201310th Int. Work. Semant. Eval. Proc.</em>, pp. 560\u2013569,\u00a02016.</p>\n<p>[7] M. Heilman, K. Collins-Thompson, and M. Eskenazi, \u201cAn analysis of statistical models and features for reading difficulty prediction,\u201d no. June, pp. 71\u201379,\u00a02010.</p>\n<p>[8] M. Xia, E. Kochmar, and T. Briscoe, \u201cText Readability Assessment for Second Language Learners,\u201d pp. 12\u201322,\u00a02016.</p>\n<p>[9] K. Collins-Thompson, \u201cComputational assessment of text readability: A survey of current and future research,\u201d <em>ITL\u200a\u2014\u200aInt. J. Appl. Linguist.</em>, vol. 165, no. 2, pp. 97\u2013135,\u00a02014.</p>\n<p>[10] L. Beinborn, T. Zesch, and I. Gurevych, \u201cTowards fine-grained readability measures for self-directed language learning,\u201d <em>Proc. 1st Work. NLP Comput. Lang. Learn.</em>, vol. 80, no. October, pp. 11\u201319,\u00a02012.</p>\n<p>[11] X. Chen and D. Meurers, \u201cCharacterizing Text Difficulty with Word Frequencies,\u201d pp. 84\u201394,\u00a02016.</p>\n<p>[12] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert, \u201cAge-of-acquisition ratings for 30,000 English words,\u201d <em>Behav. Res. Methods</em>, vol. 44, no. 4, pp. 978\u2013990,\u00a02012.</p>\n<p>[13] A. C. Graesser, D. S. McNamara, M. M. Louwerse, and Z. Cai, \u201cCoh-Metrix: Analysis of text on cohesion and language,\u201d <em>Behav. Res. Methods, Instruments, Comput.</em>, vol. 36, no. 2, pp. 193\u2013202,\u00a02004.</p>\n<p>[14] S. E. Schwarm and M. Ostendorf, \u201cReading level assessment using support vector machines and statistical language models,\u201d <em>ACL-05\u201343rd Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.</em>, no. June, pp. 523\u2013530,\u00a02005.</p>\n<p>[15] E. Pitler and A. Nenkova, \u201cRevisiting readability: A unified framework for predicting text quality,\u201d <em>EMNLP 2008\u20132008 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf. A Meet. SIGDAT, a Spec. Interes. Gr. ACL</em>, no. October, pp. 186\u2013195,\u00a02008.</p>\n<p>[16] T. V. Der Br\u00fcck, S. Hartrumpf, and H. Helbig, \u201cA readability checker with supervised learning using deep indicators,\u201d <em>Inform.</em>, vol. 32, no. 4, pp. 429\u2013435,\u00a02008.</p>\n<p>[17] R. Barzilay and M. Lapata, \u201cModeling Local Coherence: An Entity-Based Approach,\u201d <em>Math. Comput. Model.</em>,\u00a02008.</p>\n<p>[18] L. Feng, M. Jansche, M. Huenerfauth, and N. N. Elhadad, \u201cA comparison of features for automatic readability assessment,\u201d <em>Proc. 23rd Int. Conf. Comput. Linguist. Posters</em>, vol. 2, no. August, pp. 276\u2013284,\u00a02010.</p>\n<p>[19] M. Mesgar and M. Strube, \u201cLexical coherence graph modeling using word embeddings,\u201d <em>2016 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. NAACL HLT 2016\u200a\u2014\u200aProc. Conf.</em>, pp. 1414\u20131423, 2016.</p>\n<p>[20] T. Fran\u00e7ois and E. Miltsakaki, \u201cDo NLP and Machine Learning Improve Traditional Readability Formulas?,\u201d <em>Proc. First Work. Predict. Improv. Text Readability Target Read. Popul.</em>, no. Pitr, pp. 49\u201357,\u00a02012.</p>\n<p>[21] O. De Clercq and V. Hoste, \u201cAll Mixed Up? Finding the Optimal Feature Set for General Readability Prediction and Its Application to English and Dutch,\u201d <em>Comput. Linguist.</em>, 2016.</p>\n<p>[22] E. Salesky and W. Shen, \u201cExploiting Morphological, Grammatical, and Semantic Correlates for Improved Text Difficulty Assessment,\u201d pp. 155\u2013162,\u00a02015.</p>\n<p>[23] L. Forti, A. Milani, L. Piersanti, F. Santarelli, V. Santucci, and S. Spina, \u201cMeasuring Text Complexity for Italian as a Second Language Learning Purposes,\u201d pp. 360\u2013368,\u00a02019.</p>\n<p>[24] R. Reynolds, \u201cInsights from Russian second language readability classification: complexity-dependent training requirements, and feature evaluation of multiple categories,\u201d pp. 289\u2013300,\u00a02016.</p>\n<p>[25] S. Vajjala and D. Meurers, \u201cOn The Applicability of Readability Models to Web Texts,\u201d <em>Proc. 2nd Work. Predict. Improv. Text Readability Target Read. Popul.</em>, pp. 59\u201368,\u00a02013.</p>\n<p>[26] K. Collins-Thompson and J. P. Callan, \u201cA Language Modeling Approach to Predicting Reading Difficulty.,\u201d <em>Proceddings Annu. Conf. North Am. Chapter Assoc. Comput. Linguist.</em>, pp. 193\u2013200,\u00a02004.</p>\n<p>[27] L. Si and J. Callan, \u201cA statistical model for scientific readability,\u201d <em>Int. Conf. Inf. Knowl. Manag. Proc.</em>, pp. 574\u2013576,\u00a02001.</p>\n<p>[28] Z. Jiang, G. Sun, Q. Gu, T. Bai, and D. Chen, \u201cA Graph-based Readability Assessment Method using Word Coupling,\u201d no. September, pp. 411\u2013420,\u00a02015.</p>\n<p>[29] S. Vajjala and D. Meurers, \u201cReadability-based Sentence Ranking for Evaluating Text Simplification,\u201d 2016.</p>\n<p>[30] S. \u0160tajner, S. P. Ponzetto, and H. Stuckenschmidt, \u201cAutomatic assessment of absolute sentence complexity,\u201d <em>IJCAI Int. Jt. Conf. Artif. Intell.</em>, no. October, pp. 4096\u20134102, 2017.</p>\n<p>[31] S. M. Yimam <em>et al.</em>, \u201cA Report on the Complex Word Identification Shared Task 2018,\u201d pp. 66\u201378,\u00a02018.</p>\n<p>[32] S. Vajjala and D. Meurers, \u201cOn Improving the Accuracy of Readability Classification using Insights from Second Language Acquisition,\u201d in <em>The 7th Workshop on the Innovative Use of NLP for Building Educational Application</em>, 2012, pp.\u00a0163\u2013173.</p>\n<p>[33] M. Mesgar and M. Strube, \u201cA Neural Local Coherence Model for Text Quality Assessment,\u201d pp. 4328\u20134339, 2019.</p>\n<p>[34] M. Cha, Y. Gwon, and H. T. Kung, \u201cLanguage Modeling by Clustering with Word Embeddings for Text Readability Assessment,\u201d <em>Proc. 2017 ACM Conf. Inf. Knowl. Manag.\u200a\u2014\u200aCIKM \u201917</em>, pp. 2003\u20132006, 2017.</p>\n<p>[35] J. Nelson, C. Perfetti, D. Liben, and M. Liben, \u201cMeasures of Text Difficulty: Testing their Predictive Value for Grade Levels and Student Performance,\u201d p. 58,\u00a02012.</p>\n<p>[36] R. Barzilay and N. Elhadad, \u201cSentence alignment for monolingual comparable corpora,\u201d pp. 25\u201332,\u00a02003.</p>\n<p>[37] S. Vajjala and I. Lucic, \u201cOneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification,\u201d pp. 297\u2013304,\u00a02018.</p>\n<p>[38] J. Silva, R. Ribeiro, A. Adami, P. Quaresma, and A. Branco, \u201cCrawling by Readability Level,\u201d <em>Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</em>, vol. 9727, pp. 306\u2013318,\u00a02016.</p>\n<p>[39] J. Alberto, W. Filho, R. Wilkens, and A. Villavicencio, \u201cAutomatic Construction of Large Readability Corpora,\u201d pp. 164\u2013173,\u00a02016.</p>\n<p>[40] Y. Ehara, I. Sato, H. Oiwa, and H. Nakagawa, \u201cMining Words in the Minds of Second Language Learners for Learner-specific Word Difficulty,\u201d <em>J. Inf. Process.</em>, vol. 26, no. 0, pp. 267\u2013275,\u00a02018.</p>\n<h3>Something just for\u00a0you</h3>\n<a href=\"https://medium.com/media/7f5cb45fd785102f7af729c560e2de3f/href\">https://medium.com/media/7f5cb45fd785102f7af729c560e2de3f/href</a><a href=\"https://medium.com/media/8a76a1650cf30355bd307195d382558a/href\">https://medium.com/media/8a76a1650cf30355bd307195d382558a/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=71d40b677563\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/voice-tech-podcast/automated-readability-prediction-71d40b677563\">Automated Readability Prediction</a> was originally published in <a href=\"https://medium.com/voice-tech-podcast\">Voice Tech Podcast</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["speech-recognition","signal-processing","conversational-ui","naturallanguageprocessing","audio"]},{"title":"Attention in NLP","pubDate":"2018-06-22 14:47:28","link":"https://medium.com/@edloginova/attention-in-nlp-734c6fa9d983?source=rss-362da63ebee0------2","guid":"https://medium.com/p/734c6fa9d983","author":"Kate Loginova","thumbnail":"","description":"\n<p>In this post, I will describe recent work on attention in deep learning models for natural language processing. I\u2019ll start with the attention mechanism as it was introduced by Bahdanau. Then, we will go through self-attention, two-way attention, key-value-predict models and hierarchical attention.</p>\n<p>In many tasks, such as machine translation or dialogue generation, we have a sequence of words as an input (e.g., an original text in English) and would like to generate another sequence of words as an output (e.g., a translation to Korean). Neural networks, especially recurrent ones (RNN), are well suited for solving such a task. I assume that you are familiar with RNNs and <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\">LSTM</a>s. Otherwise, I recommend to check out an explanation in a famous <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">blog post</a> by Christopher Olah.</p>\n<p>The \u201csequence-to-sequence\u201d neural network models are widely used for NLP. A popular type of these models is an \u201cencoder-decoder\u201d. There, one part of the network\u200a\u2014\u200aencoder\u200a\u2014\u200aencodes the input sequence into a fixed-length context vector. This vector is an internal representation of the text. This context vector is then decoded into the output sequence by the decoder. See an\u00a0example:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*1ui7iDq956eDs-mAZHEdIg.png\"><figcaption>An encoder-decoder neural network architecture. An example on machine translation: an input sequence is an English sentence \u201cHow are you\u201d and the reply of the system would be a Korean translation: \u201c\uc798\u00a0\uc9c0\ub0c8\uc5b4\uc694\u201d.</figcaption></figure><p>Here <em>h</em> denotes hidden states of the encoder and <em>s</em> of the decoder. <em>Tx</em> and <em>Ty</em> are the lengths of the input and output word sequences respectively. <em>q</em> is a function which generates the context vector out of the encoder\u2019s hidden states. It can be, for example, just q({h_i}) = h_T. So, we take the last hidden state as an internal representation of the entire sentence.</p>\n<p>You can easily experiment with these models, as most deep learning libraries have general purpose encoder-decoder frameworks. To name a few, see Google\u2019s <a href=\"https://github.com/google/seq2seq\">implementation</a> for Tensorflow and IBM\u2019s <a href=\"https://github.com/IBM/pytorch-seq2seq\">one</a> for\u00a0PyTorch.</p>\n<p>However, there is a catch with the common encoder-decoder approach: a neural network compresses all the information of an input source sentence into a fixed-length vector. It has been shown that this leads to a decline in performance when dealing with long sentences. The attention mechanism was introduced by Bahdanau in \u201c<a href=\"https://arxiv.org/abs/1409.0473\">Neural Machine Translation by Jointly Learning to Align and Translate</a><strong>\u201d </strong>to alleviate this\u00a0problem.</p>\n<h3>Attention</h3>\n<p>The basic idea: each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence. In other words, it only pays attention to some input words. Let\u2019s investigate how this is implemented.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/710/1*9Lcq9ni9aujScFYyyHRhhA.png\"><figcaption>An illustration of the attention mechanism (RNNSearch) proposed by [<a href=\"https://arxiv.org/abs/1409.0473\">Bahdanau, 2014</a>]. Instead of converting the entire input sequence into a single context vector, we create a separate context vector for each output (target) word. These vectors consist of the weighted sums of encoder\u2019s hidden\u00a0states.</figcaption></figure><p>Encoder works as usual, and the difference is only on the decoder\u2019s part. As you can see from a picture, the decoder\u2019s hidden state is computed with a context vector, the previous output and the previous hidden state. But now we use not a single context vector c, but a separate context vector c_i for each target\u00a0word.</p>\n<p>These context vectors are computed as a weighted sum of <em>annotations</em> generated by the encoder. In Bahdanau\u2019s paper, they use a Bidirectional LSTM, so these annotations are concatenations of hidden states in forward and backward directions.</p>\n<p>The weight of each annotation is computed by an <em>alignment model </em>which scores how well the inputs and the output match. An alignment model is a feedforward neural network, for instance. In general, it can be any other model as\u00a0well.</p>\n<p>As a result, the alphas\u200a\u2014\u200athe weights of hidden states when computing a context vector\u200a\u2014\u200ashow how important a given annotation is in deciding the next state and generating the output word. These are the attention scores.</p>\n<p>If you want to read a bit more about the intuition behind this, visit <a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\">WildML\u2019s blog post</a>. You can also enjoy <a href=\"https://distill.pub/2016/augmented-rnns/\">an interactive visualization</a> in the Distill blog. In the meantime, let\u2019s move on to a bit more advanced attention mechanisms.</p>\n<h3>Memory networks</h3>\n<p>One group of attention mechanisms repeats the computation of an attention vector between the query and the context through multiple layers. It is referred to as multi-hop. They are mainly variants of end-to-end memory networks, which we will discuss\u00a0now.</p>\n<p>[<a href=\"https://arxiv.org/abs/1503.08895\">Sukhbaatar, 2015</a>] argues that the attention mechanism implemented by Bahdanau can be seen as a form of memory. They extend this mechanism to a multi-hop setting. It means that the network reads the same input sequence multiple times before producing an output, and updates the memory contents at each step. Another modification is that the model works with multiple source sentences instead of a single\u00a0one.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4KzlyesX5UJUCj9JtjtXPQ.png\"><figcaption>End-to-End Memory Networks.</figcaption></figure><p>Let\u2019s take a look at the inner workings. First, let me describe the single layer case (a). It implements a single memory hop operation. The entire input set of sentences is converted into memory vectors <em>m</em>. The query <em>q</em> is also embedded to obtain an internal state <em>u</em>. We compute the match between <em>u</em> and each memory by taking the inner product followed by a softmax. This way we obtain a probability vector <em>p </em>over the inputs (this is the attention part). Each input also has a corresponding output vector. We use the weights <em>p </em>to weigh a sum of these output vectors. This sum is a response vector <em>o </em>from the memory. Now we have an output vector <em>o</em> and the input embedding <em>u</em>. We sum them, multiply by a weight matrix <em>W</em> and apply a softmax to predict a\u00a0label.</p>\n<p>Now, we can extend the model to handle K hop operations (b). The memory layers are stacked so that the input to layers k + 1 is the sum of the output and the input from layer k. Each layer has its own embedding matrices for the\u00a0inputs.</p>\n<p>When the input and output embeddings are the same across different layers, the memory is identical to the attention mechanism of Bahdanau. The difference is that it makes multiple hops over the memory (because it tries to integrate information from multiple sentences).</p>\n<p>A fine-grained extension of this method is an Attentive Reader introduced by [<a href=\"https://arxiv.org/pdf/1506.03340.pdf\">Hermann, 2015</a>].</p>\n<h3>Variations of attention</h3>\n<p>[<a href=\"https://arxiv.org/abs/1508.04025\">Luong, 2015</a>] introduces the difference between <strong>global</strong> and <strong>local </strong>attention. The idea of a global attention is to use all the hidden states of the encoder when computing each context vector. The downside of a global attention model is that it has to attend to all words on the source side for each target word, which is computationally costly. To overcome this, the local attention first chooses a position in the source sentence. This position will determine a window of words that the model attends to. The authors also experimented with different alignment functions and simplified the computation path compared to Bahdanau\u2019s work.</p>\n<p>Attention Sum Reader [<a href=\"https://arxiv.org/abs/1603.01547\">Kadlec, 2016</a>] uses <strong>attention as a pointer </strong>over discrete tokens in the text. The task is to select an answer to a given question from the context paragraph. The difference with other methods is that the model selects the answer from the context directly using the computed attention instead of using the attention scores to weigh the sum of hidden\u00a0vectors.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iMoEyRo18sOegWVWy_1JTw.png\"><figcaption>Attention Sum\u00a0Reader.</figcaption></figure><p>As an example, let us consider the question-context pair. Let the context be \u201cA UFO was observed above our city in January and again in March.\u201d and the question be \u201cAn observer has spotted a UFO in\u00a0\u2026\u00a0.\u201d January and March are equally good candidates, so the previous models will assign equal attention scores. They would then compute a vector between the representations of these two words and propose the word with the closest word embedding as the answer. At the same time, Attention Sum Reader would correctly propose January or March, because it chooses words directly from the\u00a0passage.</p>\n<h3>Two-way Attention &amp; Coattention</h3>\n<p>As you might have noticed, in the previous model we pay attention from source to target. It makes sense in translation, but what about other fields? For example, consider textual entailment. We have a premise \u201c<em>If you help the needy, God will reward you\u201d </em>and a hypothesis \u201c<em>Giving money to a poor man has good consequences\u201d. </em>Our task is to understand whether the premise entails the hypothesis (in this case, it does). It would be useful to pay attention not only from the hypothesis to the text but also the other way\u00a0around.</p>\n<p>This brings the concept of <strong>two-way attention </strong>[<a href=\"https://arxiv.org/abs/1509.06664\">Rockt\u00e4schel, 2015</a>]. The idea is to use the same model to attend over the premise, as well as over the hypothesis. In the simplest form, you can simply swap the two sequences. This produces two attended representations which can be concatenated.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/711/1*NtpjZPqrvt32SHIcycPH2A.png\"><figcaption>Top: model from [<a href=\"https://arxiv.org/abs/1509.06664\">Rockt\u00e4schel, 2015</a>]. Bottom: MatchLSTM from [<a href=\"https://arxiv.org/abs/1512.08849\">Wang, Jiang, 2016</a>]. h vectors in the first model are weighted versions of the premise only, while in the second model they \u201crepresent the matching between the premise and the hypothesis up to position\u00a0k.\u201d</figcaption></figure><p>However, such a model will not let you emphasize more important matching results. For instance, alignment between stop words is less important than between the content words. In addition, the model still uses a single vector to represent the premise. To overcome these limitations, [<a href=\"https://arxiv.org/abs/1512.08849\">Wang, Jiang, 2016</a>] developed MatchLSTM. To deal with the importance of the matching, they add a special LSTM that will remember important matching results, while forgetting the others. This additional LSTM is also used to increase the granularity level. We will now multiply attention weights with each hidden state. It performed well in question answering and textual entailment tasks.</p>\n<p>The question answering task gave rise to even more advanced ways to combine both sides. Bahdanau\u2019s model, that we have seen in the beginning, uses a summary vector of the query to attend to the context. In contrast to it, the <strong>coattention</strong> is computed as an alignment matrix on <em>all</em> pairs of context and query words. As an example of this approach, let\u2019s examine Dynamic Coattention Networks [<a href=\"https://arxiv.org/abs/1611.01604\">Xiong,\u00a02016</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FwTl2wQKw_m765oC8ytuhg.png\"><figcaption>Dynamic Coattention Networks [<a href=\"https://arxiv.org/abs/1611.01604\">Xiong,\u00a02016</a>].</figcaption></figure><p>Let\u2019s walk through what is going on in the picture. First, we compute the affinity matrix of all pairs of document and question words. Then we get the attention weights AQ across the document for each word in the question and AD\u200a\u2014\u200athe other way around. Next, the summary or attention context of the document in light of each word in the question is computed. In the same way, we can compute it for the question in light of each word in the document. Finally, we compute the summaries of the previous attention contexts given each word in the document. The resulting vectors are concatenated into a co-dependent representation of the question and the document. This is called the<em> coattention context</em>.</p>\n<h3>Self-attention</h3>\n<p>Another problem is that recurrent network can only memorize limited passage contexts in practice despite its theoretical capabilities. For example, in question answering, one answer candidate is often unaware of the clues in other parts of the paragraph. [<a href=\"https://arxiv.org/pdf/1601.06733.pdf\">Cheng, 2016</a>] proposed a self-attention, sometimes called intra-attention. It is a mechanism relating different positions of a <em>single</em> sequence to compute its internal representation.</p>\n<p>Self-attention has been used successfully in a variety of tasks. One of the use cases is sentiment analysis. For tasks like this, standard attention is not directly applicable, because there is no extra information: the model is only given one sentence as input. A common approach is to use the final hidden state or pooling. However, it is hard to preserve semantics this way. In Question Answering, there are models used in the <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">SQuAD</a> competition: r-net [<a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">Wang, 2017</a>] and Reinforced Mnemonic Reader [<a href=\"https://arxiv.org/abs/1705.02798\">Hu, 2017</a>]. In Natural Language Inference\u200a\u2014\u200aDecomposable Attention Model [<a href=\"https://arxiv.org/pdf/1606.01933.pdf\">Parikh, 2016</a>]. <br>In machine translation, self-attention also contributes to impressive results. For example, recently a model, named Transformer, was introduced in a paper with a rather bold title \u201c<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>\u201d [<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani, 2017</a>]. As you can guess, this model relies only on self-attention without the use of RNNs. As a result, it is highly parallelizable and requires less time to train, while establishing state-of-the-art results on\u00a0WMT2014.</p>\n<p>But perhaps the most exciting property for linguists would be that self-attention seems to learn sophisticated syntactic patterns. Take a look at the example of the networks learning how to resolve anaphora in the sentence:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/765/1*9RoiZP_hPSC2rBK7f1xDjA.png\"><figcaption>Syntactic patterns learnt by the Transformer [<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani, 2017</a>] using solely self-attention.</figcaption></figure><h3>Key-Value(-Predict) attention</h3>\n<p>In the discussed algorithms, we require output vectors to simultaneously store information for predicting the next word, computing the attention, and encode content relevant to future steps. Such an overloaded use of output representations can make training unnecessarily difficult. A key-value(-predict) attention [<a href=\"https://arxiv.org/abs/1702.04521\">Daniluk, 2017</a>] has been proposed to combat the problem. In a key-value part, we separate output vectors\u00a0into</p>\n<ul>\n<li>keys to calculate the attention, and</li>\n<li>values to encode the next-word distribution and context representation.</li>\n</ul>\n<p>However, we still use the value part for two goals at once. So, the authors split it again and in the end, the model outputs three vectors at each time step. The first is used to encode the next-word distribution, the second serves as a key to compute the attention vector, and the third as value for an attention mechanism.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CsWgx8jutJ3_txYepNY-kg.png\"><figcaption>Key-value(-predict) attention. We separate output vectors into key, value (and predict) parts to lower the burden of storing information for three different goals in the same\u00a0vector.</figcaption></figure><h3>Hierarchical &amp; Nested attention</h3>\n<p>Texts tend to have a hierarchical structure and the importance of words and sentences are highly context dependent. To include this insight, hierarchical model proposed by [<a href=\"http://www.aclweb.org/anthology/N16-1174\">Yang, 2016</a>] uses two levels of attention\u200a\u2014\u200aone at the word level and one at the sentence level. Such architecture also allows for a better visualization\u200a\u2014\u200athe highly informative components of a document are highlighted. A similar idea was presented in the literature for word- and character-levels as well and adapted to a multilingual setting by [<a href=\"https://arxiv.org/pdf/1707.00896.pdf\">Pappas,\u00a02017</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1007/1*2Biacyha5OYWE6LldZHs4g.png\"></figure><p>Another take on this is Attention-over-Attention [<a href=\"https://arxiv.org/abs/1607.04423\">Cui, 2016</a>]. The idea is to place another attention over the primary attentions, to indicate the \u201cimportance\u201d of each attentions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/792/1*M3LtSKq5t1huqY7-8JRN7g.png\"><figcaption>Attention-over-Attention [<a href=\"https://arxiv.org/abs/1607.04423\">Cui,\u00a02016</a>].</figcaption></figure><p>Attention-over-Attention is motivated by the Attention Sum Reader we have seen before. They first calculate a pair-wise matching matrix M by multiplying contextual embeddings produced by an LSTM for each pair of words. Then, they apply a column-wise softmax to get the query-to-document attention weights (alphas). The weights represent an alignment between the entire document and a single query word. After that, document-to-query attention weights are obtained by applying softmax row-wise. It is again a form of two-way attention. But now we calculate the dot-product of these weights to get attention-over-attention.</p>\n<h3>Attention flow</h3>\n<p>The last of the more established concepts I would like to mention is an attention flow network. In such a network the attention model is decoupled from the RNN. An idea was introduced in the BiDAF model [<a href=\"https://arxiv.org/pdf/1611.01603.pdf\">Seo, 2016</a>]. The attention is computed for every time step, and the attended vector, along with the representations from previous layers, is allowed to flow through to the next modelling layer. The goal is to reduce the information loss caused by early summarization. A multi-hop extension is the Ruminating Reader [<a href=\"https://arxiv.org/abs/1704.07415\">Gong,\u00a02017</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KWXqGIqtIKiMSh60Wvw_vw.png\"><figcaption>BiDirectional Attention Flow Model [<a href=\"https://arxiv.org/pdf/1611.01603.pdf\">Seo,\u00a02016</a>].</figcaption></figure><p>On the image, we can see the attention flow layer. It is responsible for linking and fusing information from the context and the query words by usin two-way attention. The inputs to the layer are contextual vector representations of the context and the query. The outputs of the layer are both the query-aware representations of the context words and the contextual embeddings. Note that we do not produce a single vector, but rather let the attention information flow to the next layer at each time\u00a0step.</p>\n<h3>Classification of attention mechanisms</h3>\n<p>There have been several attempts to classify the existing attention models. I\u2019ll outline the main differences mentioned in different literature overviews.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fM7O-UzKz2O95CsuVWHZnQ.png\"><figcaption>[<a href=\"https://arxiv.org/pdf/1711.07341.pdf\">Huang, 2017</a>] distinguish between several types of attention fusion processes: (1) Word-level fusion. (2) High-level fusion. (2\u2019) High-level fusion (Alternative). (3) Self-boosted fusion. (3\u2019) Self-boosted fusion (Alternative). See the paper for more\u00a0details.</figcaption></figure><p>1. attention weights computation\u200a\u2014\u200asee [<a href=\"https://arxiv.org/abs/1611.01603\">Seo,\u00a02016</a>]</p>\n<ul>\n<li>a dynamic attention mechanism\u200a\u2014\u200athe attention weights are updated dynamically given the query and the context as well as the previous attention: RNNSearch, Attentive Reader, MatchLSTM\u00a0\u2026</li>\n<li>computes the attention weights once, which are then fed into an output layer for final prediction: Attention Sum\u00a0Reader</li>\n<li>repeats computing an attention vector between the query and the context through multiple layers: End-to-End Memory\u00a0Networks</li>\n</ul>\n<p>2. multi-pass &amp; single-pass\u200a\u2014\u200asee [<a href=\"https://arxiv.org/abs/1704.07415\">Gong,\u00a02017</a>]</p>\n<p>3. one-dimensional attention &amp; two-dimensional attention</p>\n<p>In one-dimensional attention, the whole source text is represented by one embedding vector. On the contrary, every word in the source has its own embedding vector in the situation of two-dimensional attention.</p>\n<p>4. one-way &amp; two-way attention (especially relevant for textual entailment and question answering)</p>\n<p>5. different matching and fusion functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YBSXtEEWqw61CEPdsvx2Hg.png\"><figcaption>Some of the most important papers on attention in NLP, organised by year and references to each\u00a0other.</figcaption></figure><p>I have tried to organize the papers mentioned in this post (and some other related to them) into a hierarchy of citations. You can find a clickable version <a href=\"https://www.draw.io/?lightbox=1&amp;target=blank&amp;highlight=FFFFFF&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page=1#G1sflgpGyT4pccPy_4TCZfkEiCt2WmHEy0\">here</a> as a draw.io diagram. The colors of the cells correspond to the NLP\u00a0task.</p>\n<h3>Remaining issues</h3>\n<p>Standard attention architecture does not directly model any structural dependencies that may exist among the source elements. Instead, it relies completely on the hidden layers of the neural network. However, modelling such structural dependencies has been shown to be important in many deep learning applications. It can significantly improve the results and there is quite some recent research into\u00a0this:</p>\n<ul>\n<li>Tree-to-Sequence Attentional Neural Machine Translation [<a href=\"https://arxiv.org/pdf/1603.06075.pdf\">Eriguchi, 2016</a>]</li>\n<li>Structured Attention Networks [<a href=\"https://arxiv.org/abs/1702.00887\">Kim,\u00a02017</a>]</li>\n<li>A Structured Self-attentive Sentence Embedding [<a href=\"https://arxiv.org/pdf/1703.03130.pdf\">Lin,\u00a02017</a>]</li>\n</ul>\n<p>Another thing to look out for is attentive weights that are biased towards the words in the end of the sentence (in case of uni-directional RNNs)[<a href=\"http://aclweb.org/anthology/P/P16/P16-1122.pdf\">Wang, 2016</a>].</p>\n<h3>Implementations</h3>\n<p>Finally, here are some implementations of the attention mechanisms available for Python deep learning frameworks:</p>\n<a href=\"https://medium.com/media/32e217e594ba3425ba6f58133c9bebd6/href\">https://medium.com/media/32e217e594ba3425ba6f58133c9bebd6/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=734c6fa9d983\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>In this post, I will describe recent work on attention in deep learning models for natural language processing. I\u2019ll start with the attention mechanism as it was introduced by Bahdanau. Then, we will go through self-attention, two-way attention, key-value-predict models and hierarchical attention.</p>\n<p>In many tasks, such as machine translation or dialogue generation, we have a sequence of words as an input (e.g., an original text in English) and would like to generate another sequence of words as an output (e.g., a translation to Korean). Neural networks, especially recurrent ones (RNN), are well suited for solving such a task. I assume that you are familiar with RNNs and <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\">LSTM</a>s. Otherwise, I recommend to check out an explanation in a famous <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">blog post</a> by Christopher Olah.</p>\n<p>The \u201csequence-to-sequence\u201d neural network models are widely used for NLP. A popular type of these models is an \u201cencoder-decoder\u201d. There, one part of the network\u200a\u2014\u200aencoder\u200a\u2014\u200aencodes the input sequence into a fixed-length context vector. This vector is an internal representation of the text. This context vector is then decoded into the output sequence by the decoder. See an\u00a0example:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*1ui7iDq956eDs-mAZHEdIg.png\"><figcaption>An encoder-decoder neural network architecture. An example on machine translation: an input sequence is an English sentence \u201cHow are you\u201d and the reply of the system would be a Korean translation: \u201c\uc798\u00a0\uc9c0\ub0c8\uc5b4\uc694\u201d.</figcaption></figure><p>Here <em>h</em> denotes hidden states of the encoder and <em>s</em> of the decoder. <em>Tx</em> and <em>Ty</em> are the lengths of the input and output word sequences respectively. <em>q</em> is a function which generates the context vector out of the encoder\u2019s hidden states. It can be, for example, just q({h_i}) = h_T. So, we take the last hidden state as an internal representation of the entire sentence.</p>\n<p>You can easily experiment with these models, as most deep learning libraries have general purpose encoder-decoder frameworks. To name a few, see Google\u2019s <a href=\"https://github.com/google/seq2seq\">implementation</a> for Tensorflow and IBM\u2019s <a href=\"https://github.com/IBM/pytorch-seq2seq\">one</a> for\u00a0PyTorch.</p>\n<p>However, there is a catch with the common encoder-decoder approach: a neural network compresses all the information of an input source sentence into a fixed-length vector. It has been shown that this leads to a decline in performance when dealing with long sentences. The attention mechanism was introduced by Bahdanau in \u201c<a href=\"https://arxiv.org/abs/1409.0473\">Neural Machine Translation by Jointly Learning to Align and Translate</a><strong>\u201d </strong>to alleviate this\u00a0problem.</p>\n<h3>Attention</h3>\n<p>The basic idea: each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence. In other words, it only pays attention to some input words. Let\u2019s investigate how this is implemented.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/710/1*9Lcq9ni9aujScFYyyHRhhA.png\"><figcaption>An illustration of the attention mechanism (RNNSearch) proposed by [<a href=\"https://arxiv.org/abs/1409.0473\">Bahdanau, 2014</a>]. Instead of converting the entire input sequence into a single context vector, we create a separate context vector for each output (target) word. These vectors consist of the weighted sums of encoder\u2019s hidden\u00a0states.</figcaption></figure><p>Encoder works as usual, and the difference is only on the decoder\u2019s part. As you can see from a picture, the decoder\u2019s hidden state is computed with a context vector, the previous output and the previous hidden state. But now we use not a single context vector c, but a separate context vector c_i for each target\u00a0word.</p>\n<p>These context vectors are computed as a weighted sum of <em>annotations</em> generated by the encoder. In Bahdanau\u2019s paper, they use a Bidirectional LSTM, so these annotations are concatenations of hidden states in forward and backward directions.</p>\n<p>The weight of each annotation is computed by an <em>alignment model </em>which scores how well the inputs and the output match. An alignment model is a feedforward neural network, for instance. In general, it can be any other model as\u00a0well.</p>\n<p>As a result, the alphas\u200a\u2014\u200athe weights of hidden states when computing a context vector\u200a\u2014\u200ashow how important a given annotation is in deciding the next state and generating the output word. These are the attention scores.</p>\n<p>If you want to read a bit more about the intuition behind this, visit <a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\">WildML\u2019s blog post</a>. You can also enjoy <a href=\"https://distill.pub/2016/augmented-rnns/\">an interactive visualization</a> in the Distill blog. In the meantime, let\u2019s move on to a bit more advanced attention mechanisms.</p>\n<h3>Memory networks</h3>\n<p>One group of attention mechanisms repeats the computation of an attention vector between the query and the context through multiple layers. It is referred to as multi-hop. They are mainly variants of end-to-end memory networks, which we will discuss\u00a0now.</p>\n<p>[<a href=\"https://arxiv.org/abs/1503.08895\">Sukhbaatar, 2015</a>] argues that the attention mechanism implemented by Bahdanau can be seen as a form of memory. They extend this mechanism to a multi-hop setting. It means that the network reads the same input sequence multiple times before producing an output, and updates the memory contents at each step. Another modification is that the model works with multiple source sentences instead of a single\u00a0one.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4KzlyesX5UJUCj9JtjtXPQ.png\"><figcaption>End-to-End Memory Networks.</figcaption></figure><p>Let\u2019s take a look at the inner workings. First, let me describe the single layer case (a). It implements a single memory hop operation. The entire input set of sentences is converted into memory vectors <em>m</em>. The query <em>q</em> is also embedded to obtain an internal state <em>u</em>. We compute the match between <em>u</em> and each memory by taking the inner product followed by a softmax. This way we obtain a probability vector <em>p </em>over the inputs (this is the attention part). Each input also has a corresponding output vector. We use the weights <em>p </em>to weigh a sum of these output vectors. This sum is a response vector <em>o </em>from the memory. Now we have an output vector <em>o</em> and the input embedding <em>u</em>. We sum them, multiply by a weight matrix <em>W</em> and apply a softmax to predict a\u00a0label.</p>\n<p>Now, we can extend the model to handle K hop operations (b). The memory layers are stacked so that the input to layers k + 1 is the sum of the output and the input from layer k. Each layer has its own embedding matrices for the\u00a0inputs.</p>\n<p>When the input and output embeddings are the same across different layers, the memory is identical to the attention mechanism of Bahdanau. The difference is that it makes multiple hops over the memory (because it tries to integrate information from multiple sentences).</p>\n<p>A fine-grained extension of this method is an Attentive Reader introduced by [<a href=\"https://arxiv.org/pdf/1506.03340.pdf\">Hermann, 2015</a>].</p>\n<h3>Variations of attention</h3>\n<p>[<a href=\"https://arxiv.org/abs/1508.04025\">Luong, 2015</a>] introduces the difference between <strong>global</strong> and <strong>local </strong>attention. The idea of a global attention is to use all the hidden states of the encoder when computing each context vector. The downside of a global attention model is that it has to attend to all words on the source side for each target word, which is computationally costly. To overcome this, the local attention first chooses a position in the source sentence. This position will determine a window of words that the model attends to. The authors also experimented with different alignment functions and simplified the computation path compared to Bahdanau\u2019s work.</p>\n<p>Attention Sum Reader [<a href=\"https://arxiv.org/abs/1603.01547\">Kadlec, 2016</a>] uses <strong>attention as a pointer </strong>over discrete tokens in the text. The task is to select an answer to a given question from the context paragraph. The difference with other methods is that the model selects the answer from the context directly using the computed attention instead of using the attention scores to weigh the sum of hidden\u00a0vectors.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iMoEyRo18sOegWVWy_1JTw.png\"><figcaption>Attention Sum\u00a0Reader.</figcaption></figure><p>As an example, let us consider the question-context pair. Let the context be \u201cA UFO was observed above our city in January and again in March.\u201d and the question be \u201cAn observer has spotted a UFO in\u00a0\u2026\u00a0.\u201d January and March are equally good candidates, so the previous models will assign equal attention scores. They would then compute a vector between the representations of these two words and propose the word with the closest word embedding as the answer. At the same time, Attention Sum Reader would correctly propose January or March, because it chooses words directly from the\u00a0passage.</p>\n<h3>Two-way Attention &amp; Coattention</h3>\n<p>As you might have noticed, in the previous model we pay attention from source to target. It makes sense in translation, but what about other fields? For example, consider textual entailment. We have a premise \u201c<em>If you help the needy, God will reward you\u201d </em>and a hypothesis \u201c<em>Giving money to a poor man has good consequences\u201d. </em>Our task is to understand whether the premise entails the hypothesis (in this case, it does). It would be useful to pay attention not only from the hypothesis to the text but also the other way\u00a0around.</p>\n<p>This brings the concept of <strong>two-way attention </strong>[<a href=\"https://arxiv.org/abs/1509.06664\">Rockt\u00e4schel, 2015</a>]. The idea is to use the same model to attend over the premise, as well as over the hypothesis. In the simplest form, you can simply swap the two sequences. This produces two attended representations which can be concatenated.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/711/1*NtpjZPqrvt32SHIcycPH2A.png\"><figcaption>Top: model from [<a href=\"https://arxiv.org/abs/1509.06664\">Rockt\u00e4schel, 2015</a>]. Bottom: MatchLSTM from [<a href=\"https://arxiv.org/abs/1512.08849\">Wang, Jiang, 2016</a>]. h vectors in the first model are weighted versions of the premise only, while in the second model they \u201crepresent the matching between the premise and the hypothesis up to position\u00a0k.\u201d</figcaption></figure><p>However, such a model will not let you emphasize more important matching results. For instance, alignment between stop words is less important than between the content words. In addition, the model still uses a single vector to represent the premise. To overcome these limitations, [<a href=\"https://arxiv.org/abs/1512.08849\">Wang, Jiang, 2016</a>] developed MatchLSTM. To deal with the importance of the matching, they add a special LSTM that will remember important matching results, while forgetting the others. This additional LSTM is also used to increase the granularity level. We will now multiply attention weights with each hidden state. It performed well in question answering and textual entailment tasks.</p>\n<p>The question answering task gave rise to even more advanced ways to combine both sides. Bahdanau\u2019s model, that we have seen in the beginning, uses a summary vector of the query to attend to the context. In contrast to it, the <strong>coattention</strong> is computed as an alignment matrix on <em>all</em> pairs of context and query words. As an example of this approach, let\u2019s examine Dynamic Coattention Networks [<a href=\"https://arxiv.org/abs/1611.01604\">Xiong,\u00a02016</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FwTl2wQKw_m765oC8ytuhg.png\"><figcaption>Dynamic Coattention Networks [<a href=\"https://arxiv.org/abs/1611.01604\">Xiong,\u00a02016</a>].</figcaption></figure><p>Let\u2019s walk through what is going on in the picture. First, we compute the affinity matrix of all pairs of document and question words. Then we get the attention weights AQ across the document for each word in the question and AD\u200a\u2014\u200athe other way around. Next, the summary or attention context of the document in light of each word in the question is computed. In the same way, we can compute it for the question in light of each word in the document. Finally, we compute the summaries of the previous attention contexts given each word in the document. The resulting vectors are concatenated into a co-dependent representation of the question and the document. This is called the<em> coattention context</em>.</p>\n<h3>Self-attention</h3>\n<p>Another problem is that recurrent network can only memorize limited passage contexts in practice despite its theoretical capabilities. For example, in question answering, one answer candidate is often unaware of the clues in other parts of the paragraph. [<a href=\"https://arxiv.org/pdf/1601.06733.pdf\">Cheng, 2016</a>] proposed a self-attention, sometimes called intra-attention. It is a mechanism relating different positions of a <em>single</em> sequence to compute its internal representation.</p>\n<p>Self-attention has been used successfully in a variety of tasks. One of the use cases is sentiment analysis. For tasks like this, standard attention is not directly applicable, because there is no extra information: the model is only given one sentence as input. A common approach is to use the final hidden state or pooling. However, it is hard to preserve semantics this way. In Question Answering, there are models used in the <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">SQuAD</a> competition: r-net [<a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">Wang, 2017</a>] and Reinforced Mnemonic Reader [<a href=\"https://arxiv.org/abs/1705.02798\">Hu, 2017</a>]. In Natural Language Inference\u200a\u2014\u200aDecomposable Attention Model [<a href=\"https://arxiv.org/pdf/1606.01933.pdf\">Parikh, 2016</a>]. <br>In machine translation, self-attention also contributes to impressive results. For example, recently a model, named Transformer, was introduced in a paper with a rather bold title \u201c<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>\u201d [<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani, 2017</a>]. As you can guess, this model relies only on self-attention without the use of RNNs. As a result, it is highly parallelizable and requires less time to train, while establishing state-of-the-art results on\u00a0WMT2014.</p>\n<p>But perhaps the most exciting property for linguists would be that self-attention seems to learn sophisticated syntactic patterns. Take a look at the example of the networks learning how to resolve anaphora in the sentence:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/765/1*9RoiZP_hPSC2rBK7f1xDjA.png\"><figcaption>Syntactic patterns learnt by the Transformer [<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani, 2017</a>] using solely self-attention.</figcaption></figure><h3>Key-Value(-Predict) attention</h3>\n<p>In the discussed algorithms, we require output vectors to simultaneously store information for predicting the next word, computing the attention, and encode content relevant to future steps. Such an overloaded use of output representations can make training unnecessarily difficult. A key-value(-predict) attention [<a href=\"https://arxiv.org/abs/1702.04521\">Daniluk, 2017</a>] has been proposed to combat the problem. In a key-value part, we separate output vectors\u00a0into</p>\n<ul>\n<li>keys to calculate the attention, and</li>\n<li>values to encode the next-word distribution and context representation.</li>\n</ul>\n<p>However, we still use the value part for two goals at once. So, the authors split it again and in the end, the model outputs three vectors at each time step. The first is used to encode the next-word distribution, the second serves as a key to compute the attention vector, and the third as value for an attention mechanism.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CsWgx8jutJ3_txYepNY-kg.png\"><figcaption>Key-value(-predict) attention. We separate output vectors into key, value (and predict) parts to lower the burden of storing information for three different goals in the same\u00a0vector.</figcaption></figure><h3>Hierarchical &amp; Nested attention</h3>\n<p>Texts tend to have a hierarchical structure and the importance of words and sentences are highly context dependent. To include this insight, hierarchical model proposed by [<a href=\"http://www.aclweb.org/anthology/N16-1174\">Yang, 2016</a>] uses two levels of attention\u200a\u2014\u200aone at the word level and one at the sentence level. Such architecture also allows for a better visualization\u200a\u2014\u200athe highly informative components of a document are highlighted. A similar idea was presented in the literature for word- and character-levels as well and adapted to a multilingual setting by [<a href=\"https://arxiv.org/pdf/1707.00896.pdf\">Pappas,\u00a02017</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1007/1*2Biacyha5OYWE6LldZHs4g.png\"></figure><p>Another take on this is Attention-over-Attention [<a href=\"https://arxiv.org/abs/1607.04423\">Cui, 2016</a>]. The idea is to place another attention over the primary attentions, to indicate the \u201cimportance\u201d of each attentions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/792/1*M3LtSKq5t1huqY7-8JRN7g.png\"><figcaption>Attention-over-Attention [<a href=\"https://arxiv.org/abs/1607.04423\">Cui,\u00a02016</a>].</figcaption></figure><p>Attention-over-Attention is motivated by the Attention Sum Reader we have seen before. They first calculate a pair-wise matching matrix M by multiplying contextual embeddings produced by an LSTM for each pair of words. Then, they apply a column-wise softmax to get the query-to-document attention weights (alphas). The weights represent an alignment between the entire document and a single query word. After that, document-to-query attention weights are obtained by applying softmax row-wise. It is again a form of two-way attention. But now we calculate the dot-product of these weights to get attention-over-attention.</p>\n<h3>Attention flow</h3>\n<p>The last of the more established concepts I would like to mention is an attention flow network. In such a network the attention model is decoupled from the RNN. An idea was introduced in the BiDAF model [<a href=\"https://arxiv.org/pdf/1611.01603.pdf\">Seo, 2016</a>]. The attention is computed for every time step, and the attended vector, along with the representations from previous layers, is allowed to flow through to the next modelling layer. The goal is to reduce the information loss caused by early summarization. A multi-hop extension is the Ruminating Reader [<a href=\"https://arxiv.org/abs/1704.07415\">Gong,\u00a02017</a>].</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KWXqGIqtIKiMSh60Wvw_vw.png\"><figcaption>BiDirectional Attention Flow Model [<a href=\"https://arxiv.org/pdf/1611.01603.pdf\">Seo,\u00a02016</a>].</figcaption></figure><p>On the image, we can see the attention flow layer. It is responsible for linking and fusing information from the context and the query words by usin two-way attention. The inputs to the layer are contextual vector representations of the context and the query. The outputs of the layer are both the query-aware representations of the context words and the contextual embeddings. Note that we do not produce a single vector, but rather let the attention information flow to the next layer at each time\u00a0step.</p>\n<h3>Classification of attention mechanisms</h3>\n<p>There have been several attempts to classify the existing attention models. I\u2019ll outline the main differences mentioned in different literature overviews.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fM7O-UzKz2O95CsuVWHZnQ.png\"><figcaption>[<a href=\"https://arxiv.org/pdf/1711.07341.pdf\">Huang, 2017</a>] distinguish between several types of attention fusion processes: (1) Word-level fusion. (2) High-level fusion. (2\u2019) High-level fusion (Alternative). (3) Self-boosted fusion. (3\u2019) Self-boosted fusion (Alternative). See the paper for more\u00a0details.</figcaption></figure><p>1. attention weights computation\u200a\u2014\u200asee [<a href=\"https://arxiv.org/abs/1611.01603\">Seo,\u00a02016</a>]</p>\n<ul>\n<li>a dynamic attention mechanism\u200a\u2014\u200athe attention weights are updated dynamically given the query and the context as well as the previous attention: RNNSearch, Attentive Reader, MatchLSTM\u00a0\u2026</li>\n<li>computes the attention weights once, which are then fed into an output layer for final prediction: Attention Sum\u00a0Reader</li>\n<li>repeats computing an attention vector between the query and the context through multiple layers: End-to-End Memory\u00a0Networks</li>\n</ul>\n<p>2. multi-pass &amp; single-pass\u200a\u2014\u200asee [<a href=\"https://arxiv.org/abs/1704.07415\">Gong,\u00a02017</a>]</p>\n<p>3. one-dimensional attention &amp; two-dimensional attention</p>\n<p>In one-dimensional attention, the whole source text is represented by one embedding vector. On the contrary, every word in the source has its own embedding vector in the situation of two-dimensional attention.</p>\n<p>4. one-way &amp; two-way attention (especially relevant for textual entailment and question answering)</p>\n<p>5. different matching and fusion functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YBSXtEEWqw61CEPdsvx2Hg.png\"><figcaption>Some of the most important papers on attention in NLP, organised by year and references to each\u00a0other.</figcaption></figure><p>I have tried to organize the papers mentioned in this post (and some other related to them) into a hierarchy of citations. You can find a clickable version <a href=\"https://www.draw.io/?lightbox=1&amp;target=blank&amp;highlight=FFFFFF&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page=1#G1sflgpGyT4pccPy_4TCZfkEiCt2WmHEy0\">here</a> as a draw.io diagram. The colors of the cells correspond to the NLP\u00a0task.</p>\n<h3>Remaining issues</h3>\n<p>Standard attention architecture does not directly model any structural dependencies that may exist among the source elements. Instead, it relies completely on the hidden layers of the neural network. However, modelling such structural dependencies has been shown to be important in many deep learning applications. It can significantly improve the results and there is quite some recent research into\u00a0this:</p>\n<ul>\n<li>Tree-to-Sequence Attentional Neural Machine Translation [<a href=\"https://arxiv.org/pdf/1603.06075.pdf\">Eriguchi, 2016</a>]</li>\n<li>Structured Attention Networks [<a href=\"https://arxiv.org/abs/1702.00887\">Kim,\u00a02017</a>]</li>\n<li>A Structured Self-attentive Sentence Embedding [<a href=\"https://arxiv.org/pdf/1703.03130.pdf\">Lin,\u00a02017</a>]</li>\n</ul>\n<p>Another thing to look out for is attentive weights that are biased towards the words in the end of the sentence (in case of uni-directional RNNs)[<a href=\"http://aclweb.org/anthology/P/P16/P16-1122.pdf\">Wang, 2016</a>].</p>\n<h3>Implementations</h3>\n<p>Finally, here are some implementations of the attention mechanisms available for Python deep learning frameworks:</p>\n<a href=\"https://medium.com/media/32e217e594ba3425ba6f58133c9bebd6/href\">https://medium.com/media/32e217e594ba3425ba6f58133c9bebd6/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=734c6fa9d983\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["machine-learning","neural-networks","nlp","attention","deep-learning"]}]}